{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project\n",
    "-------\n",
    "\n",
    "### Stage 2 - Modelling phase\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages and data\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible packages that need to be installed:\n",
    "\n",
    "1. SpaCy\n",
    "\n",
    "<code> conda install -c spacy spacy </code>\n",
    "\n",
    "2. 'en_core_web_md' - library used in SpaCy\n",
    "\n",
    "<code> python -m spacy download en_core_web_md </code>\n",
    "\n",
    "3. wordcloud\n",
    "\n",
    "<code> conda install -c conda-forge wordcloud </code>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nattiechan/anaconda3/envs/myenv/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "\n",
    "# Basics\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import numpy as np\n",
    "\n",
    "# Graphs\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Preprocessing; model selection and evaluation\n",
    "from sklearn import pipeline, preprocessing\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# text handling\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# for custom countvectorizer with SpaCy lemmatization\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, VectorizerMixin\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# WordCloud\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "data = pd.read_csv(\"saved_csv/df.csv\")\n",
    "data.drop(columns = \"Unnamed: 0\",inplace=True)\n",
    "\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "------\n",
    "\n",
    "Below outlines the questions to be answered with the cleaned data, and proposed methodology for each question.\n",
    "\n",
    "<b><i><u>Question #1</b></i></u>\n",
    "\n",
    "**What are some of the qualitative insights for the tech industry to improve MH support for employees?**\n",
    "\n",
    "In the survey, there is a question asking participants to briefly describe what the industry can do as a whole to improve mental health (MH) support for their employees. The purpose of the analysis is to see whether there are any *underlying themes or patterns* to the responses.\n",
    "\n",
    "<i><u>Methodology</i></u>\n",
    "\n",
    "Process the text using *SpaCy Lemmatizer* and *CountVectorizer*, then visualize the patterns using WordCloud. Since the font on the WordCloud is based on the frequency of the words appearing in the entire dataset, the more frequently used words will be larger in the WordCloud, suggesting a pattern or theme.\n",
    "\n",
    "<b><i><u>Question #2</b></i></u>\n",
    "\n",
    "**What are the factors that affect comfort level in discussing MH at workplace?**\n",
    "\n",
    "One proposed hypothesis is that improving comfort level in discussing MH at workplace can contribute to improving MH support for employees in the tech industry. A *prescriptive analysis* will be done to determine the factors using various questions pertaining to employer's MH coverage, personal MH status and experience with having MH conversations at the workplace, and overall perceived ratings in the dataset as features.\n",
    "\n",
    "<i><u>Methodology</i></u>\n",
    "\n",
    "Determine the statistical significance of each feature and the model as a whole using *Statsmodels Logistic Regression*, then use either *Random Forrest Classifier* or *XGBoost Classifier* to determine feature importance.\n",
    "\n",
    "<b><i><u>Question #3</b></i></u>\n",
    "\n",
    "**Can we predict one's comfort level in discussing MH at workplace using participants' qualitative responses of ways to improve MH support?**\n",
    "\n",
    "A *predictive model* will be built to determine if the qualitative responses are viable predictors of one's comfort level in discussing MH at workplace.\n",
    "\n",
    "<i><u>Methodology</i></u>\n",
    "\n",
    "Design a model using either *Convolutional Neural Network (CNN)* or a *Recurrent Neural Network (RNN)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "------\n",
    "\n",
    "#### Independent Variables for Q2\n",
    "------\n",
    "\n",
    "Questions will be grouped into one of the following categories:\n",
    "\n",
    "- Current employer's MH coverage\n",
    "- Previous employer's MH coverage\n",
    "- MH status\n",
    "- Witnessed experience (of discussing MH in the workplace)\n",
    "- Overall perceived ratings\n",
    "- Comfort level discussing MH in the workplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting questions into categories\n",
    "\n",
    "current_mh_coverage = [\"Does your employer provide mental health benefits as part of healthcare coverage?\",\n",
    "               \"Do you know the options for mental health care available under your employer-provided health coverage?\",\n",
    "               \"Has your employer ever formally discussed mental health (for example, as part of a wellness campaign or other official communication)?\",\n",
    "               \"Does your employer offer resources to learn more about mental health disorders and options for seeking help?\",\n",
    "               \"Is your anonymity protected if you choose to take advantage of mental health or substance abuse treatment resources provided by your employer?\",\n",
    "               \"If a mental health issue prompted you to request a medical leave from work, how easy or difficult would it be to ask for that leave?\"]\n",
    "\n",
    "previous_mh_coverage = [\"Have your previous employers provided mental health benefits?\",\n",
    "                        \"Were you aware of the options for mental health care provided by your previous employers?\",\n",
    "                        \"Did your previous employers ever formally discuss mental health (as part of a wellness campaign or other official communication)?\",\n",
    "                        \"Did your previous employers provide resources to learn more about mental health disorders and how to seek help?\",\n",
    "                        \"Was your anonymity protected if you chose to take advantage of mental health or substance abuse treatment resources with previous employers?\"]\n",
    "\n",
    "mh_status = [\"Do you currently have a mental health disorder?\",\n",
    "             \"Have you ever been diagnosed with a mental health disorder?\",'Anxiety Disorder', 'Mood Disorder', \n",
    "             'Psychotic Disorder','Eating Disorder', 'Neurodevelopmental Disorders','Personality Disorder', \n",
    "             'Obsessive-Compulsive Disorder','Post-Traumatic Stress Disorder', 'Dissociative Disorder',\n",
    "             'Substance-Related and Addictive Disorders', 'Other','Adjustment disorder',\n",
    "             \"Have you had a mental health disorder in the past?\",\n",
    "             \"Have you ever sought treatment for a mental health disorder from a mental health professional?\",\n",
    "             \"Do you have a family history of mental illness?\",\n",
    "             \"How willing would you be to share with friends and family that you have a mental illness?\",\n",
    "             \"Would you be willing to bring up a physical health issue with a potential employer in an interview?\"]\n",
    "\n",
    "witnessed_exp = [\"Have your observations of how another individual who discussed a mental health issue made you less likely to reveal a mental health issue yourself in your current workplace?\",\n",
    "                 \"Have you observed or experienced an unsupportive or badly handled response to a mental health issue in your current or previous workplace?\",\n",
    "                 \"Have you observed or experienced supportive or well handled response to a mental health issue in your current or previous workplace?\"]\n",
    "\n",
    "ratings = df.columns[df.columns.str.contains(\"Overall\")]\n",
    "\n",
    "demographics = [\"What is your age?\",\"What is your gender?\",\"What country do you live in?\",\n",
    "                \"What US state or territory do you live in?\",\"What is your race?\"]\n",
    "\n",
    "comfort_talking_current = [\"Would you feel comfortable discussing a mental health issue with your direct supervisor(s)?\",\n",
    "                           \"Have you ever discussed your mental health with your employer?\",\n",
    "                           \"Would you feel comfortable discussing a mental health issue with your coworkers?\",\n",
    "                           \"Have you ever discussed your mental health with coworkers?\",\n",
    "                           \"Have you ever had a coworker discuss their or another coworker's mental health with you?\",\n",
    "                           \"Would you feel more comfortable talking to your coworkers about your physical health or your mental health?\",\n",
    "                           \"Would you bring up your mental health with a potential employer in an interview?\",\n",
    "                           \"Are you openly identified at work as a person with a mental health issue?\"]\n",
    "\n",
    "comfort_talking_previous = [\"Would you have felt more comfortable talking to your previous employer about your physical health or your mental health?\",\n",
    "                            \"Would you have been willing to discuss your mental health with your direct supervisor(s)?\",\n",
    "                            \"Did you ever discuss your mental health with your previous employer?\",\n",
    "                            \"Would you have been willing to discuss your mental health with your coworkers at previous employers?\",\n",
    "                            \"Did you ever discuss your mental health with a previous coworker(s)?\",\n",
    "                            \"Did you ever have a previous coworker discuss their or another coworker's mental health with you?\",\n",
    "                            \"Would you bring up your mental health with a potential employer in an interview?\",\n",
    "                            \"Are you openly identified at work as a person with a mental health issue?\"]\n",
    "\n",
    "comfort_dependent_var = [\"Would you feel comfortable discussing a mental health issue with your direct supervisor(s)?\",\n",
    "                           \"Have you ever discussed your mental health with your employer?\",\n",
    "                           \"Would you feel comfortable discussing a mental health issue with your coworkers?\",\n",
    "                           \"Have you ever discussed your mental health with coworkers?\",\n",
    "                           \"Have you ever had a coworker discuss their or another coworker's mental health with you?\",\n",
    "                           \"Would you feel more comfortable talking to your coworkers about your physical health or your mental health?\",\n",
    "                           \"Would you have felt more comfortable talking to your previous employer about your physical health or your mental health?\",\n",
    "                           \"Would you have been willing to discuss your mental health with your direct supervisor(s)?\",\n",
    "                           \"Did you ever discuss your mental health with your previous employer?\",\n",
    "                           \"Would you have been willing to discuss your mental health with your coworkers at previous employers?\",\n",
    "                           \"Did you ever discuss your mental health with a previous coworker(s)?\",\n",
    "                           \"Did you ever have a previous coworker discuss their or another coworker's mental health with you?\",\n",
    "                           \"Would you bring up your mental health with a potential employer in an interview?\",\n",
    "                           \"Are you openly identified at work as a person with a mental health issue?\"]\n",
    "\n",
    "categories = [current_mh_coverage,previous_mh_coverage,mh_status,witnessed_exp,\n",
    "              ratings,comfort_talking_current,comfort_talking_previous,comfort_dependent_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various questions from current employer's MH coverage, MH status, witnessed experience as well as overall perceived ratings will be chosen as indpendent variables.\n",
    "\n",
    "Dummy variables will be generated from the responses of each question. Responses will be grouped in two general categories (eg. Yes/No) and for the independent variables, one group of responses will be dropped to avoid multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \"Would you feel comfortable discussing a mental health issue with your coworkers?\"\n",
    "\n",
    "2. \"Would you feel comfortable discussing a mental health issue with your direct supervisor(s)?\"\n",
    "\n",
    "3. \"Have you ever discussed your mental health with your employer?\"\n",
    "\n",
    "4. \"Have you ever discussed your mental health with coworkers?\"\n",
    "\n",
    "5. \"Are you openly identified at work as a person with a mental health issue?\"\n",
    "\n",
    "6. \"Would you feel more comfortable talking to your coworkers about your physical health or your mental health?\",\n",
    "\n",
    "7. \"Have you ever had a coworker discuss their or another coworker's mental health with you?\"\n",
    "\n",
    "8. \"Would you bring up your mental health with a potential employer in an interview?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating functions for dummy generation\n",
    "def make_dummies(question,columns_to_keep = \"Yes\"):\n",
    "    '''\n",
    "    The function creates dummy variables for independent variables.\n",
    "    You can specify the column to KEEP.\n",
    "    \n",
    "    Inputs:\n",
    "    ------\n",
    "    question: the question/column name that you wish to create a dummy vairable for\n",
    "    columns_to_keep: a string indicating the answer you want to keep\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    result: a DataFrame of the desire column in dummy variables.\n",
    "    '''\n",
    "    \n",
    "    dummies = pd.get_dummies(df_2.loc[:,question])\n",
    "    for j in range(len(dummies.columns)):\n",
    "        name = question + \"__\" + columns_to_keep\n",
    "        dummies.rename(columns = {columns_to_keep : name},inplace=True)\n",
    "\n",
    "    result = dummies.loc[:,[name]]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def make_dummies_iloc(question,list_to_keep):\n",
    "    '''\n",
    "    The function creates dummy variables for independent variables.\n",
    "    This is designed for instances when you need to keep multiple responses.\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    question: the question/column name that you wish to create a dummy variable for\n",
    "    list_to_keep: a list of numbers corresponding the column numbers of the responses \n",
    "                  (after it has been converted to dummy variables) that you want to keep\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    result: A DataFrame of the desire column in dummy variables.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    dummies = pd.get_dummies(df_2.loc[:,question])\n",
    "    for j in range(len(dummies.columns)):\n",
    "        name = question + \"__\" + dummies.columns[j]\n",
    "        dummies.rename(columns = {dummies.columns[j] : name},inplace=True)\n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    for num in list_to_keep:\n",
    "        to_keep = dummies.iloc[:,num]\n",
    "        result = pd.concat([result,to_keep],axis=1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a copy of the data\n",
    "df_2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up 2 dataframes for concatenating data\n",
    "omitted = pd.DataFrame(columns = [\"Question\",\"Answer\"])\n",
    "final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# current_mh_coverage\n",
    "for i in [0,2,3]:\n",
    "    result = make_dummies(current_mh_coverage[i],\"Yes\")\n",
    "    final = pd.concat([final,result],axis = 1)\n",
    "\n",
    "result = make_dummies_iloc(current_mh_coverage[5],[2,-2,-1])\n",
    "final = pd.concat([final,result],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# witnessed_exp\n",
    "result = make_dummies(witnessed_exp[0],\"Yes\")\n",
    "final = pd.concat([final,result],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mh_status\n",
    "\n",
    "# Modifying some responses for ease of sorting\n",
    "old_answer_1 = \"Possibly\"\n",
    "old_answer_2 = \"-1\"\n",
    "answer = \"Don't Know\"\n",
    "\n",
    "to_dummy = []\n",
    "for i in [0,1,-5,-3,-1]:\n",
    "    to_dummy.append(mh_status[i])\n",
    "\n",
    "for num in [0,2]:\n",
    "    df_2.loc[:,to_dummy[num]][df_2.loc[:,to_dummy[num]]==old_answer_1]=answer\n",
    "\n",
    "df_2.loc[:,to_dummy[2]][df_2.loc[:,to_dummy[2]]==old_answer_2]=answer\n",
    "\n",
    "#creating dummy variables\n",
    "result = make_dummies(to_dummy[0],\"Yes\")\n",
    "final = pd.concat([final,result],axis = 1)\n",
    "\n",
    "for i in [2,3,4,5,6,7,8,9,10,11,12,13,15,17]:\n",
    "    final = pd.concat([final,df_2.loc[:,mh_status[i]]],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings\n",
    "for i in [1,4]:\n",
    "    final = pd.concat([final,df_2.loc[:,ratings[i]]],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizing the table of independent variables\n",
    "independent_q1 = final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependent Variables for Q2\n",
    "------\n",
    "\n",
    "Various questions from questions from comfort level of discussing MH in the workplace will be chosen as the dependent variable.\n",
    "\n",
    "Dummy variables will be generated from the responses of each question. Clusters of classes will be created afterwards with unsupervised clustering methods using the dummy variables as independent variables. In contrast to the independent variable, all responses can be kept since there is no issue of multicollinearity with dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Would you feel comfortable discussing a mental health issue with your coworkers?\"\n",
    "\n",
    "answers = [\"Maybe\",\"No\",\"Not Applicable\",\"Yes\"]\n",
    "\n",
    "dep = df_2[question].copy()\n",
    "\n",
    "for num in range(len(answers)):\n",
    "    if num != 3:\n",
    "        dep[dep==answers[num]] = 0 #Hesitant\n",
    "    else:\n",
    "        dep[dep==answers[num]] = 1 #Comfortable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Would you feel comfortable discussing a mental health issue with your direct supervisor(s)?\"\n",
    "\n",
    "answers = [\"Maybe\",\"No\",\"Not Applicable\",\"Yes\"]\n",
    "\n",
    "dep_2 = df_2[question].copy()\n",
    "\n",
    "for num in range(len(answers)):\n",
    "    if num != 3:\n",
    "        dep_2[dep_2==answers[num]] = 0 #Hesitant\n",
    "    else:\n",
    "        dep_2[dep_2==answers[num]] = 1 #Comfortable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a table of dependent variables after turning the responses into dummy variables\n",
    "dependent = df.loc[:,comfort_dependent_var]\n",
    "\n",
    "num_list = [0,2,5,6,7,9,12]\n",
    "columns_to_join = [1,3,4,8,10,11,13]\n",
    "\n",
    "final_dep = pd.get_dummies(dependent.iloc[:,num_list])\n",
    "\n",
    "for i in columns_to_join:\n",
    "    result = pd.get_dummies(dependent.iloc[:,i])\n",
    "    final_dep = pd.concat([final_dep,result],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting classes\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans Clustering will be used to generate the clusters. The goal is to generate 4 clusters with a relatively normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KMeans Clustering\n",
    "X = final_dep\n",
    "\n",
    "inertia = []\n",
    "\n",
    "for num in range(1,21):\n",
    "    k_means_model = KMeans(n_clusters = num)\n",
    "    k_means_model.fit(X)\n",
    "\n",
    "    y_pred = k_means_model.predict(X)\n",
    "    inertia.append(k_means_model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(1,21),inertia)\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.xticks(np.arange(0,21,2))\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Inertia of KMeans model by number of clusters\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inertia is a metric that can be used to determine the optimal number of clusters for KMeans Clustering. As we can see with the graph illustrating the inertia at different number of clusters, 4 clusters would be an appropriate choice to avoid overfitting since the curve is starting to flatten out at around that mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the classes\n",
    "k_means_model = KMeans(n_clusters = 4)\n",
    "k_means_model.fit(X)\n",
    "\n",
    "y_pred = k_means_model.predict(X)\n",
    "\n",
    "# Visualizing the results\n",
    "clusters = pd.DataFrame(y_pred)\n",
    "clusters.rename(columns = {0 : \"Class\"},inplace=True)\n",
    "clusters[\"Class\"].groupby(clusters[\"Class\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annotate the data points with the KMeans prediction\n",
    "Y = k_means_model.predict(X)\n",
    "\n",
    "#Plot\n",
    "plt.scatter(X.iloc[:, 5], X.iloc[:, 6], c=Y, edgecolor='k')\n",
    "\n",
    "#Plot the K centers\n",
    "plt.scatter(k_means_model.cluster_centers_[:, 0],k_means_model.cluster_centers_[:, 1], c='red',marker=\"*\",s=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Method source: https://medium.com/@davidmasse8/unsupervised-learning-for-categorical-data-dd7e497033ae\n",
    "# Code source: https://github.com/nicodv/kmodes\n",
    "from kmodes.kmodes import KModes\n",
    "# define the k-modes model\n",
    "km = KModes(n_clusters=4, init='Huang', n_init=5, verbose=0)\n",
    "\n",
    "X = final_dep\n",
    "# # fit the clusters to the skills dataframe\n",
    "clusters = km.fit_predict(X)\n",
    "# get an array of cluster modes\n",
    "kmodes = km.cluster_centroids_\n",
    "shape = kmodes.shape\n",
    "# For each cluster mode (a vector of \"1\" and \"0\")\n",
    "# find and print the column headings where \"1\" appears.\n",
    "# If no \"1\" appears, assign to \"no-skills\" cluster.\n",
    "for i in range(shape[0]):\n",
    "    if sum(kmodes[i,:]) == 0:\n",
    "        print(\"\\ncluster \" + str(i) + \": \")\n",
    "        print(\"no-skills cluster\")\n",
    "    else:\n",
    "        print(\"\\ncluster \" + str(i) + \": \")\n",
    "        cent = kmodes[i,:]\n",
    "        for j in final_dep.columns[np.nonzero(cent)]:\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the results\n",
    "table = pd.DataFrame(clusters)\n",
    "table.rename(columns = {0 : \"Class\"},inplace=True)\n",
    "table[\"Class\"].groupby(table[\"Class\"]).count()\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table.to_csv(\"saved_csv/q1_dependent_alt.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is an element of randomness in KMeans Clustering, the code will generate different counts with each iteration. Since the goal is to generate a cluster of 4 classes with an approximately normal distribution, the following cluster count will be used for the analysis:\n",
    "\n",
    "|Class|Count|\n",
    "|------|------|\n",
    "|0|143|\n",
    "|1|490|\n",
    "|2|386|\n",
    "|3|154|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### What are some of the qualitative insights for the tech industry to improve MH support for employees?\n",
    "-----\n",
    "\n",
    "Qualitative responses from the question \"briefly describe what you think the industry as a whole and/or employers could do to improve mental health support for employees\" will be processed using [spacy-vectorizers](https://github.com/mpavlovic/spacy-vectorizers), a custom CountVectorizer with SpaCy lemmatization embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using spacy-vectorizers to process texts\n",
    "# Code source: https://github.com/mpavlovic/spacy-vectorizers\n",
    "\n",
    "class SpacyPipeInitializer(object):\n",
    "    def __init__(self, nlp, join_str=\" \", batch_size=10000, n_threads=2):\n",
    "        self.nlp = nlp\n",
    "        self.join_str = join_str\n",
    "        self.batch_size = batch_size\n",
    "        self.n_threads = n_threads\n",
    "        \n",
    "class SpacyPipeProcessor(SpacyPipeInitializer):\n",
    "    def __init__(self, nlp, multi_iters=False, join_str=\" \", batch_size=10000, n_threads=2):\n",
    "        super(SpacyPipeProcessor, self).__init__(nlp, join_str, batch_size, n_threads)\n",
    "        self.multi_iters = multi_iters\n",
    "    \n",
    "    def __call__(self, raw_documents):\n",
    "        docs_generator = self.nlp.pipe(raw_documents, batch_size=self.batch_size, n_threads=self.n_threads)\n",
    "        return docs_generator if self.multi_iters == False else list(docs_generator)\n",
    "    \n",
    "class SpacyLemmaCountVectorizer(CountVectorizer):\n",
    "    \n",
    "    def __init__(self, input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None,\n",
    "                 lowercase=True, preprocessor=None, tokenizer=None,\n",
    "                 stop_words=None, token_pattern=r\"(?u)[^\\r\\n ]+\",\n",
    "                 ngram_range=(1, 1), analyzer='word',\n",
    "                 max_df=1.0, min_df=1, max_features=None,\n",
    "                 vocabulary=None, binary=False, dtype=np.int64, \n",
    "                 nlp=None, ignore_chars='!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~', \n",
    "                 join_str=\" \", use_pron=False):\n",
    "        \n",
    "        super().__init__(input, encoding, decode_error, strip_accents, \n",
    "                                                   lowercase, preprocessor, tokenizer,\n",
    "                                                   stop_words, token_pattern, ngram_range, \n",
    "                                                   analyzer, max_df, min_df, max_features,\n",
    "                                                   vocabulary, binary, dtype)\n",
    "        self.ignore_chars = ignore_chars\n",
    "        self.join_str = ' ' # lemmas have to be joined for splitting\n",
    "        self.use_pron = use_pron\n",
    "        self.translate_table = dict((ord(char), None) for char in self.ignore_chars)\n",
    "        \n",
    "    def lemmatize_from_docs(self, docs):\n",
    "        for doc in docs:\n",
    "            lemmas_gen = (token.lemma_.translate(self.translate_table) if self.use_pron or token.lemma_!='-PRON-' else token.lower_.translate(self.translate_table) for token in doc)  # generator expression\n",
    "            yield self.join_str.join(lemmas_gen) if self.join_str is not None else [lemma for lemma in lemmas_gen]\n",
    "    \n",
    "    def build_tokenizer(self):\n",
    "        return lambda doc: doc.split()\n",
    "    \n",
    "    def transform(self, spacy_docs):\n",
    "        raw_documents = self.lemmatize_from_docs(spacy_docs)\n",
    "        return super(SpacyLemmaCountVectorizer, self).transform(raw_documents)\n",
    "    \n",
    "    def fit_transform(self, spacy_docs, y=None):\n",
    "        raw_documents = self.lemmatize_from_docs(spacy_docs)\n",
    "        return super(SpacyLemmaCountVectorizer, self).fit_transform(raw_documents, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grabbing the text responses\n",
    "corpus = df.iloc[:,-9]\n",
    "\n",
    "# customization stopwords to filter out some words\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"mental\",\"health\",\"issue\",\"work\",\n",
    "                  \"take\",\"hour\",\"tech\",\"industry\",\"people\",\"employee\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer with SpaCy Lemmatization\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "spp = SpacyPipeProcessor(nlp, n_threads=1, multi_iters=True)\n",
    "spacy_docs = spp(corpus);\n",
    "\n",
    "slcv = SpacyLemmaCountVectorizer(min_df=3,stop_words=stopwords, ngram_range=(1, 3), ignore_chars='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "slcv.fit(spacy_docs)\n",
    "count_vectors = slcv.transform(spacy_docs); count_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the text has been processed, a WordCloud is generated to visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pulling out the list of parsed words and put them into a wordcloud\n",
    "list_of_words = slcv.vocabulary_.keys()\n",
    "list_of_words = list(list_of_words)\n",
    "list_of_words.sort()\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(\" \".join(list_of_words))\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words with bigger fonts in the WordCloud are ones that occur in the dataset with higher frequency. From the WordCloud above, it seems like words like \"open\", \"support\", \"talk\" are big themes of the responses. This finding supports the need of looking deeper into factos that affect comfort level in discussing MH in the workplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #2\n",
    "\n",
    "### What are the factors that affect comfort level in discussing MH at workplace?\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that creates dummy variables\n",
    "def make_dummies_q(column_num,column_name):\n",
    "    '''\n",
    "    This function creates a dummy variable for a particular column,\n",
    "    and drops a column to avoid multicollinearity since the responses will be included in the independent variables.\n",
    "    \n",
    "    Inputs:\n",
    "    ------\n",
    "    column_num = an integer of the number of column/question you wish to create a dummy variable for\n",
    "    column_name = the response that you wish to drop to avoid multicollinearity\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    result = a DataFrame of dummy variables, containing n-1 answers\n",
    "    '''\n",
    "\n",
    "    result = pd.get_dummies(df_2.iloc[:,column_num])\n",
    "    result.drop(columns = column_name, inplace=True)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing the dependent variables (after they have been clustered)\n",
    "dependent_class = pd.read_csv(\"saved_csv/q1_dependent_alt.csv\")\n",
    "dependent_class.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplifing dependent variables to 0/1\n",
    "dependent_class_alt = dependent_class.copy()\n",
    "\n",
    "dependent_class_alt[dependent_class_alt < 2] = 0\n",
    "dependent_class_alt[dependent_class_alt >= 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What is your gender?\n",
    "gender = make_dummies_q(-7,\"Male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping countries into 5 continents\n",
    "country_names = df_2.iloc[:,-6].groupby(df_2.iloc[:,-6]).count().index\n",
    "\n",
    "df_2[\"countries_continent\"] = df_2.iloc[:,-6]\n",
    "\n",
    "north_am = [8,34,55]\n",
    "south_am = [0,6,9,56]\n",
    "asia = [3,20,23,24,26,28,29,39,43,44,46]\n",
    "africa = [14,30,33,37,48]\n",
    "europe = [2,4,5,7,10,11,13,15,16,17,18,19,21,22,25,27,31,32,35,38,40,41,42,45,47,49,50,51,52,53,54]\n",
    "oceania = [1,36]\n",
    "did_not_answer = [12]\n",
    "\n",
    "continents = [north_am,south_am,asia,africa,europe,oceania,did_not_answer]\n",
    "names = [\"North America\", \"South America\", \"Asia\", \"Africa\", \"Europe\", \"Oceania\", \"Did not answer\"]\n",
    "\n",
    "for position,continent in enumerate(continents):\n",
    "    for num in np.flip(continent):\n",
    "        df_2.loc[:,\"countries_continent\"][df_2.loc[:,\"countries_continent\"]== country_names[num]]=names[position]\n",
    "        \n",
    "countries = make_dummies_q(-1,\"North America\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is your race?\n",
    "races = make_dummies_q(-5,\"Did not answer\")\n",
    "for num in range(-4,0):\n",
    "    races.drop(columns = [races.columns[num]],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_size\n",
    "company_size = make_dummies_q(2,\"0\")\n",
    "\n",
    "# select companies with larger size into the independent variables\n",
    "size = company_size.iloc[:,[3,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a table of demographics\n",
    "demographics = pd.concat([gender,countries,races,size,df_2.iloc[:,[-9,1,3,4]]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating features/independent variables\n",
    "independent_q1_alt = pd.concat([independent_q1,demographics],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.42130938710965604\n",
      "            Iterations: 104\n",
      "            Function evaluations: 105\n",
      "            Gradient evaluations: 104\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>y</td>        <th>  No. Observations:  </th>  <td>  1173</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1124</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>    48</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Mon, 25 Mar 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.2790</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>12:22:01</td>     <th>  Log-Likelihood:    </th> <td> -494.20</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -685.44</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>1.154e-53</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   -3.8876</td> <td> 4080.380</td> <td>   -0.001</td> <td> 0.999</td> <td>-8001.286</td> <td> 7993.510</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.1786</td> <td>    0.100</td> <td>    1.793</td> <td> 0.073</td> <td>   -0.017</td> <td>    0.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.2870</td> <td>    0.093</td> <td>    3.095</td> <td> 0.002</td> <td>    0.105</td> <td>    0.469</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.1175</td> <td>    0.098</td> <td>   -1.198</td> <td> 0.231</td> <td>   -0.310</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.1880</td> <td>    0.084</td> <td>    2.240</td> <td> 0.025</td> <td>    0.023</td> <td>    0.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.2833</td> <td>    0.091</td> <td>    3.101</td> <td> 0.002</td> <td>    0.104</td> <td>    0.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.5327</td> <td>    0.090</td> <td>    5.951</td> <td> 0.000</td> <td>    0.357</td> <td>    0.708</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -0.1864</td> <td>    0.093</td> <td>   -1.998</td> <td> 0.046</td> <td>   -0.369</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    0.0125</td> <td>    0.116</td> <td>    0.108</td> <td> 0.914</td> <td>   -0.214</td> <td>    0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.0592</td> <td>    0.096</td> <td>    0.617</td> <td> 0.537</td> <td>   -0.129</td> <td>    0.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.1871</td> <td>    0.101</td> <td>   -1.854</td> <td> 0.064</td> <td>   -0.385</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>   -0.0229</td> <td>    0.072</td> <td>   -0.319</td> <td> 0.749</td> <td>   -0.163</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>    0.0273</td> <td>    0.081</td> <td>    0.337</td> <td> 0.736</td> <td>   -0.132</td> <td>    0.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>    0.0224</td> <td>    0.082</td> <td>    0.271</td> <td> 0.786</td> <td>   -0.139</td> <td>    0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>   -0.3378</td> <td>    0.125</td> <td>   -2.711</td> <td> 0.007</td> <td>   -0.582</td> <td>   -0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   -0.0117</td> <td>    0.083</td> <td>   -0.141</td> <td> 0.888</td> <td>   -0.174</td> <td>    0.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>    0.0215</td> <td>    0.087</td> <td>    0.246</td> <td> 0.806</td> <td>   -0.150</td> <td>    0.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>    0.0534</td> <td>    0.087</td> <td>    0.616</td> <td> 0.538</td> <td>   -0.117</td> <td>    0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>   -0.0278</td> <td>    0.086</td> <td>   -0.324</td> <td> 0.746</td> <td>   -0.196</td> <td>    0.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.0130</td> <td>    0.083</td> <td>    0.157</td> <td> 0.875</td> <td>   -0.149</td> <td>    0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>   -0.0133</td> <td>    0.092</td> <td>   -0.145</td> <td> 0.885</td> <td>   -0.194</td> <td>    0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>    0.2724</td> <td>    0.111</td> <td>    2.452</td> <td> 0.014</td> <td>    0.055</td> <td>    0.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    0.7081</td> <td>    0.103</td> <td>    6.884</td> <td> 0.000</td> <td>    0.506</td> <td>    0.910</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>    0.1272</td> <td>    0.093</td> <td>    1.369</td> <td> 0.171</td> <td>   -0.055</td> <td>    0.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    0.0402</td> <td>    0.089</td> <td>    0.451</td> <td> 0.652</td> <td>   -0.134</td> <td>    0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>   -0.0367</td> <td>    0.105</td> <td>   -0.351</td> <td> 0.725</td> <td>   -0.242</td> <td>    0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.0477</td> <td>    0.088</td> <td>   -0.545</td> <td> 0.586</td> <td>   -0.219</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>    0.0208</td> <td>    0.075</td> <td>    0.276</td> <td> 0.782</td> <td>   -0.127</td> <td>    0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   -0.0753</td> <td>    0.078</td> <td>   -0.965</td> <td> 0.335</td> <td>   -0.228</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>   -0.0599</td> <td>    0.090</td> <td>   -0.668</td> <td> 0.504</td> <td>   -0.236</td> <td>    0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>   -0.0525</td> <td>    0.098</td> <td>   -0.537</td> <td> 0.592</td> <td>   -0.244</td> <td>    0.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>    0.2890</td> <td>    0.109</td> <td>    2.657</td> <td> 0.008</td> <td>    0.076</td> <td>    0.502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>   -0.1810</td> <td> 2.06e+04</td> <td>-8.78e-06</td> <td> 1.000</td> <td>-4.04e+04</td> <td> 4.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    0.3777</td> <td>    0.153</td> <td>    2.472</td> <td> 0.013</td> <td>    0.078</td> <td>    0.677</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>    0.1391</td> <td>    0.093</td> <td>    1.499</td> <td> 0.134</td> <td>   -0.043</td> <td>    0.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>    0.0382</td> <td>    0.107</td> <td>    0.357</td> <td> 0.721</td> <td>   -0.172</td> <td>    0.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   -0.0211</td> <td>    0.077</td> <td>   -0.275</td> <td> 0.783</td> <td>   -0.171</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>    0.5800</td> <td>  750.323</td> <td>    0.001</td> <td> 0.999</td> <td>-1470.026</td> <td> 1471.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>    0.1404</td> <td>    0.096</td> <td>    1.469</td> <td> 0.142</td> <td>   -0.047</td> <td>    0.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>    0.0793</td> <td>    0.067</td> <td>    1.182</td> <td> 0.237</td> <td>   -0.052</td> <td>    0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>    0.2360</td> <td>    0.164</td> <td>    1.440</td> <td> 0.150</td> <td>   -0.085</td> <td>    0.557</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>    0.0113</td> <td>    0.077</td> <td>    0.148</td> <td> 0.882</td> <td>   -0.139</td> <td>    0.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>   -0.9064</td> <td> 1.38e+05</td> <td>-6.57e-06</td> <td> 1.000</td> <td> -2.7e+05</td> <td>  2.7e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>    0.0574</td> <td>    0.074</td> <td>    0.774</td> <td> 0.439</td> <td>   -0.088</td> <td>    0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>   -0.2026</td> <td>    0.088</td> <td>   -2.298</td> <td> 0.022</td> <td>   -0.375</td> <td>   -0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>   -0.2329</td> <td>    0.092</td> <td>   -2.536</td> <td> 0.011</td> <td>   -0.413</td> <td>   -0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>   -6.2620</td> <td>  622.745</td> <td>   -0.010</td> <td> 0.992</td> <td>-1226.821</td> <td> 1214.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>    0.2739</td> <td>    0.154</td> <td>    1.775</td> <td> 0.076</td> <td>   -0.029</td> <td>    0.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>    0.1079</td> <td>    0.245</td> <td>    0.440</td> <td> 0.660</td> <td>   -0.373</td> <td>    0.588</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.15 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 1173\n",
       "Model:                          Logit   Df Residuals:                     1124\n",
       "Method:                           MLE   Df Model:                           48\n",
       "Date:                Mon, 25 Mar 2019   Pseudo R-squ.:                  0.2790\n",
       "Time:                        12:22:01   Log-Likelihood:                -494.20\n",
       "converged:                       True   LL-Null:                       -685.44\n",
       "                                        LLR p-value:                 1.154e-53\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -3.8876   4080.380     -0.001      0.999   -8001.286    7993.510\n",
       "x1             0.1786      0.100      1.793      0.073      -0.017       0.374\n",
       "x2             0.2870      0.093      3.095      0.002       0.105       0.469\n",
       "x3            -0.1175      0.098     -1.198      0.231      -0.310       0.075\n",
       "x4             0.1880      0.084      2.240      0.025       0.023       0.353\n",
       "x5             0.2833      0.091      3.101      0.002       0.104       0.462\n",
       "x6             0.5327      0.090      5.951      0.000       0.357       0.708\n",
       "x7            -0.1864      0.093     -1.998      0.046      -0.369      -0.004\n",
       "x8             0.0125      0.116      0.108      0.914      -0.214       0.239\n",
       "x9             0.0592      0.096      0.617      0.537      -0.129       0.247\n",
       "x10           -0.1871      0.101     -1.854      0.064      -0.385       0.011\n",
       "x11           -0.0229      0.072     -0.319      0.749      -0.163       0.117\n",
       "x12            0.0273      0.081      0.337      0.736      -0.132       0.186\n",
       "x13            0.0224      0.082      0.271      0.786      -0.139       0.184\n",
       "x14           -0.3378      0.125     -2.711      0.007      -0.582      -0.094\n",
       "x15           -0.0117      0.083     -0.141      0.888      -0.174       0.150\n",
       "x16            0.0215      0.087      0.246      0.806      -0.150       0.193\n",
       "x17            0.0534      0.087      0.616      0.538      -0.117       0.223\n",
       "x18           -0.0278      0.086     -0.324      0.746      -0.196       0.140\n",
       "x19            0.0130      0.083      0.157      0.875      -0.149       0.175\n",
       "x20           -0.0133      0.092     -0.145      0.885      -0.194       0.167\n",
       "x21            0.2724      0.111      2.452      0.014       0.055       0.490\n",
       "x22            0.7081      0.103      6.884      0.000       0.506       0.910\n",
       "x23            0.1272      0.093      1.369      0.171      -0.055       0.309\n",
       "x24            0.0402      0.089      0.451      0.652      -0.134       0.215\n",
       "x25           -0.0367      0.105     -0.351      0.725      -0.242       0.168\n",
       "x26           -0.0477      0.088     -0.545      0.586      -0.219       0.124\n",
       "x27            0.0208      0.075      0.276      0.782      -0.127       0.168\n",
       "x28           -0.0753      0.078     -0.965      0.335      -0.228       0.078\n",
       "x29           -0.0599      0.090     -0.668      0.504      -0.236       0.116\n",
       "x30           -0.0525      0.098     -0.537      0.592      -0.244       0.139\n",
       "x31            0.2890      0.109      2.657      0.008       0.076       0.502\n",
       "x32           -0.1810   2.06e+04  -8.78e-06      1.000   -4.04e+04    4.04e+04\n",
       "x33            0.3777      0.153      2.472      0.013       0.078       0.677\n",
       "x34            0.1391      0.093      1.499      0.134      -0.043       0.321\n",
       "x35            0.0382      0.107      0.357      0.721      -0.172       0.248\n",
       "x36           -0.0211      0.077     -0.275      0.783      -0.171       0.129\n",
       "x37            0.5800    750.323      0.001      0.999   -1470.026    1471.186\n",
       "x38            0.1404      0.096      1.469      0.142      -0.047       0.328\n",
       "x39            0.0793      0.067      1.182      0.237      -0.052       0.211\n",
       "x40            0.2360      0.164      1.440      0.150      -0.085       0.557\n",
       "x41            0.0113      0.077      0.148      0.882      -0.139       0.161\n",
       "x42           -0.9064   1.38e+05  -6.57e-06      1.000    -2.7e+05     2.7e+05\n",
       "x43            0.0574      0.074      0.774      0.439      -0.088       0.203\n",
       "x44           -0.2026      0.088     -2.298      0.022      -0.375      -0.030\n",
       "x45           -0.2329      0.092     -2.536      0.011      -0.413      -0.053\n",
       "x46           -6.2620    622.745     -0.010      0.992   -1226.821    1214.296\n",
       "x47            0.2739      0.154      1.775      0.076      -0.029       0.576\n",
       "x48            0.1079      0.245      0.440      0.660      -0.373       0.588\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.15 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p-value WITH DEMOGRAPHICS\n",
    "\n",
    "X_1 = independent_q1_alt\n",
    "Y_1 = dep.values.astype(int)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_1)\n",
    "X_transformed_1 = scaler.transform(X_1)\n",
    "\n",
    "X_transformed_1 = np.hstack([np.ones([X_transformed_1.shape[0],1]), X_transformed_1])\n",
    "\n",
    "logit = sm.Logit(Y_1, X_transformed_1)\n",
    "fitted_model_demo = logit.fit_regularized()\n",
    "fitted_model_demo.summary()\n",
    "\n",
    "# alpha = 0.05\n",
    "# all features together are significant\n",
    "# Columns that are significant: 1,3,4,5,6,13,20,21,30,32,43,44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model to get the feature importances\n",
    "\n",
    "# filter depreciation warning that is associated with not using \n",
    "# the most updated version of numpy and scikit-learnin the vitual environment\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "ind_q1 = independent_q1_alt.copy()\n",
    "ind_q1.columns = [np.arange(len(ind_q1.columns))]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ind_q1,dep.values.astype(int),test_size = 0.2)\n",
    "\n",
    "estimators = [(\"normalize\", preprocessing.StandardScaler()),\n",
    "             (\"model\",LogisticRegression())]\n",
    "\n",
    "pipe = pipeline.Pipeline(estimators)\n",
    "\n",
    "param_grid = [{\"model\":[XGBClassifier()], \n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__max_depth\":[1,2,3,4,5],\"model__n_estimators\":[50,100,150,200],\"model__n_jobs\":[6]},\n",
    "              {\"model\": [RandomForestClassifier()],\n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__n_estimators\":[100,150,200],\"model__n_jobs\":[6]}]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=6)\n",
    "fitted_grid_1 = grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('normalize', StandardScaler(copy=True, with_mean=True, with_std=True)), ('model', XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=1, min_child_weight=1, missing=None, n_estimators=150,\n",
       "       n_jobs=6, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_grid_1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7739872068230277"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_grid_1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7872340425531915"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_grid_1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['How willing would you be to share with friends and family that you have a mental illness?',\n",
       "       'What is your age?',\n",
       "       'If a mental health issue prompted you to request a medical leave from work, how easy or difficult would it be to ask for that leave?__Very easy',\n",
       "       'Has your employer ever formally discussed mental health (for example, as part of a wellness campaign or other official communication)?__Yes',\n",
       "       'More than 1000',\n",
       "       'If a mental health issue prompted you to request a medical leave from work, how easy or difficult would it be to ask for that leave?__Somewhat easy',\n",
       "       'Have you ever sought treatment for a mental health disorder from a mental health professional?',\n",
       "       'Europe',\n",
       "       'If a mental health issue prompted you to request a medical leave from work, how easy or difficult would it be to ask for that leave?__Neither easy nor difficult',\n",
       "       'Have your observations of how another individual who discussed a mental health issue made you less likely to reveal a mental health issue yourself in your current workplace?__Yes',\n",
       "       'Personality Disorder', 'Asia'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ranking of factors that are important for predicting comfort level, from most to least important\n",
    "indices = np.flip(fitted_grid_1.best_estimator_.named_steps[\"model\"].feature_importances_.argsort())\n",
    "\n",
    "# creating a list of factors that are statistically significant\n",
    "sig_list = [1,3,4,5,6,13,20,21,30,32,43,44]\n",
    "\n",
    "numbers = []\n",
    "for position,idx in enumerate(indices):\n",
    "    for num in sig_list:\n",
    "        if idx == num:\n",
    "            numbers.append(idx)\n",
    "\n",
    "independent_q1_alt.columns[numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.4524186983610468\n",
      "            Iterations: 101\n",
      "            Function evaluations: 101\n",
      "            Gradient evaluations: 101\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>y</td>        <th>  No. Observations:  </th>  <td>  1173</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1124</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>    48</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Mon, 25 Mar 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.2865</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>11:55:13</td>     <th>  Log-Likelihood:    </th> <td> -530.69</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -743.83</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>4.269e-62</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   -3.7444</td> <td> 6.79e+04</td> <td>-5.51e-05</td> <td> 1.000</td> <td>-1.33e+05</td> <td> 1.33e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.0873</td> <td>    0.094</td> <td>   -0.929</td> <td> 0.353</td> <td>   -0.272</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.0602</td> <td>    0.089</td> <td>   -0.678</td> <td> 0.498</td> <td>   -0.234</td> <td>    0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.1002</td> <td>    0.093</td> <td>    1.081</td> <td> 0.280</td> <td>   -0.081</td> <td>    0.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.2839</td> <td>    0.078</td> <td>    3.654</td> <td> 0.000</td> <td>    0.132</td> <td>    0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.3212</td> <td>    0.085</td> <td>    3.778</td> <td> 0.000</td> <td>    0.155</td> <td>    0.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.4869</td> <td>    0.086</td> <td>    5.653</td> <td> 0.000</td> <td>    0.318</td> <td>    0.656</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -0.1611</td> <td>    0.087</td> <td>   -1.856</td> <td> 0.063</td> <td>   -0.331</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    0.0635</td> <td>    0.113</td> <td>    0.565</td> <td> 0.572</td> <td>   -0.157</td> <td>    0.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.0192</td> <td>    0.091</td> <td>    0.210</td> <td> 0.833</td> <td>   -0.160</td> <td>    0.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.2863</td> <td>    0.098</td> <td>   -2.911</td> <td> 0.004</td> <td>   -0.479</td> <td>   -0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    0.0358</td> <td>    0.074</td> <td>    0.485</td> <td> 0.627</td> <td>   -0.109</td> <td>    0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>   -0.0919</td> <td>    0.081</td> <td>   -1.141</td> <td> 0.254</td> <td>   -0.250</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -0.0722</td> <td>    0.082</td> <td>   -0.886</td> <td> 0.376</td> <td>   -0.232</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>    0.0429</td> <td>    0.079</td> <td>    0.541</td> <td> 0.589</td> <td>   -0.113</td> <td>    0.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   -0.0679</td> <td>    0.080</td> <td>   -0.845</td> <td> 0.398</td> <td>   -0.225</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>   -0.1785</td> <td>    0.091</td> <td>   -1.972</td> <td> 0.049</td> <td>   -0.356</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>    0.0805</td> <td>    0.089</td> <td>    0.904</td> <td> 0.366</td> <td>   -0.094</td> <td>    0.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>    0.1084</td> <td>    0.079</td> <td>    1.376</td> <td> 0.169</td> <td>   -0.046</td> <td>    0.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.1328</td> <td>    0.143</td> <td>    0.927</td> <td> 0.354</td> <td>   -0.148</td> <td>    0.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>   -0.1978</td> <td>    0.098</td> <td>   -2.012</td> <td> 0.044</td> <td>   -0.390</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>    0.2140</td> <td>    0.106</td> <td>    2.024</td> <td> 0.043</td> <td>    0.007</td> <td>    0.421</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    0.6088</td> <td>    0.093</td> <td>    6.561</td> <td> 0.000</td> <td>    0.427</td> <td>    0.791</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>    0.4051</td> <td>    0.091</td> <td>    4.463</td> <td> 0.000</td> <td>    0.227</td> <td>    0.583</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    0.2062</td> <td>    0.086</td> <td>    2.395</td> <td> 0.017</td> <td>    0.037</td> <td>    0.375</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>   -0.1476</td> <td>    0.106</td> <td>   -1.386</td> <td> 0.166</td> <td>   -0.356</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.0678</td> <td>    0.083</td> <td>   -0.818</td> <td> 0.413</td> <td>   -0.230</td> <td>    0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>   -0.0394</td> <td>    0.078</td> <td>   -0.506</td> <td> 0.613</td> <td>   -0.192</td> <td>    0.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   -2.2808</td> <td> 1.04e+06</td> <td> -2.2e-06</td> <td> 1.000</td> <td>-2.03e+06</td> <td> 2.03e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>    0.1247</td> <td>    0.095</td> <td>    1.310</td> <td> 0.190</td> <td>   -0.062</td> <td>    0.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>    0.1042</td> <td>    0.073</td> <td>    1.431</td> <td> 0.152</td> <td>   -0.038</td> <td>    0.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>    0.0626</td> <td>    0.107</td> <td>    0.586</td> <td> 0.558</td> <td>   -0.147</td> <td>    0.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>   -0.1430</td> <td> 2.15e+04</td> <td>-6.64e-06</td> <td> 1.000</td> <td>-4.22e+04</td> <td> 4.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    0.0102</td> <td>    0.139</td> <td>    0.074</td> <td> 0.941</td> <td>   -0.261</td> <td>    0.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>    0.0607</td> <td>    0.087</td> <td>    0.698</td> <td> 0.485</td> <td>   -0.110</td> <td>    0.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>    0.0398</td> <td>    0.087</td> <td>    0.459</td> <td> 0.646</td> <td>   -0.130</td> <td>    0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   -0.2354</td> <td>    0.114</td> <td>   -2.070</td> <td> 0.038</td> <td>   -0.458</td> <td>   -0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>    0.4981</td> <td>  339.695</td> <td>    0.001</td> <td> 0.999</td> <td> -665.292</td> <td>  666.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>    0.0512</td> <td>    0.092</td> <td>    0.556</td> <td> 0.578</td> <td>   -0.129</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>    0.0431</td> <td>    0.082</td> <td>    0.526</td> <td> 0.599</td> <td>   -0.118</td> <td>    0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>    0.2837</td> <td>    0.147</td> <td>    1.934</td> <td> 0.053</td> <td>   -0.004</td> <td>    0.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>    0.0669</td> <td>    0.070</td> <td>    0.951</td> <td> 0.341</td> <td>   -0.071</td> <td>    0.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>   -0.9019</td> <td> 1.14e+05</td> <td>-7.94e-06</td> <td> 1.000</td> <td>-2.23e+05</td> <td> 2.23e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>    0.0169</td> <td>    0.073</td> <td>    0.231</td> <td> 0.818</td> <td>   -0.126</td> <td>    0.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>   -0.1055</td> <td>    0.082</td> <td>   -1.284</td> <td> 0.199</td> <td>   -0.267</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>   -0.0223</td> <td>    0.084</td> <td>   -0.265</td> <td> 0.791</td> <td>   -0.187</td> <td>    0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>   -6.7142</td> <td>  800.043</td> <td>   -0.008</td> <td> 0.993</td> <td>-1574.770</td> <td> 1561.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>   -0.0284</td> <td>    0.142</td> <td>   -0.200</td> <td> 0.842</td> <td>   -0.307</td> <td>    0.250</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>    0.2534</td> <td>    0.237</td> <td>    1.068</td> <td> 0.286</td> <td>   -0.212</td> <td>    0.718</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.15 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 1173\n",
       "Model:                          Logit   Df Residuals:                     1124\n",
       "Method:                           MLE   Df Model:                           48\n",
       "Date:                Mon, 25 Mar 2019   Pseudo R-squ.:                  0.2865\n",
       "Time:                        11:55:13   Log-Likelihood:                -530.69\n",
       "converged:                       True   LL-Null:                       -743.83\n",
       "                                        LLR p-value:                 4.269e-62\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -3.7444   6.79e+04  -5.51e-05      1.000   -1.33e+05    1.33e+05\n",
       "x1            -0.0873      0.094     -0.929      0.353      -0.272       0.097\n",
       "x2            -0.0602      0.089     -0.678      0.498      -0.234       0.114\n",
       "x3             0.1002      0.093      1.081      0.280      -0.081       0.282\n",
       "x4             0.2839      0.078      3.654      0.000       0.132       0.436\n",
       "x5             0.3212      0.085      3.778      0.000       0.155       0.488\n",
       "x6             0.4869      0.086      5.653      0.000       0.318       0.656\n",
       "x7            -0.1611      0.087     -1.856      0.063      -0.331       0.009\n",
       "x8             0.0635      0.113      0.565      0.572      -0.157       0.284\n",
       "x9             0.0192      0.091      0.210      0.833      -0.160       0.198\n",
       "x10           -0.2863      0.098     -2.911      0.004      -0.479      -0.094\n",
       "x11            0.0358      0.074      0.485      0.627      -0.109       0.180\n",
       "x12           -0.0919      0.081     -1.141      0.254      -0.250       0.066\n",
       "x13           -0.0722      0.082     -0.886      0.376      -0.232       0.088\n",
       "x14            0.0429      0.079      0.541      0.589      -0.113       0.198\n",
       "x15           -0.0679      0.080     -0.845      0.398      -0.225       0.090\n",
       "x16           -0.1785      0.091     -1.972      0.049      -0.356      -0.001\n",
       "x17            0.0805      0.089      0.904      0.366      -0.094       0.255\n",
       "x18            0.1084      0.079      1.376      0.169      -0.046       0.263\n",
       "x19            0.1328      0.143      0.927      0.354      -0.148       0.414\n",
       "x20           -0.1978      0.098     -2.012      0.044      -0.390      -0.005\n",
       "x21            0.2140      0.106      2.024      0.043       0.007       0.421\n",
       "x22            0.6088      0.093      6.561      0.000       0.427       0.791\n",
       "x23            0.4051      0.091      4.463      0.000       0.227       0.583\n",
       "x24            0.2062      0.086      2.395      0.017       0.037       0.375\n",
       "x25           -0.1476      0.106     -1.386      0.166      -0.356       0.061\n",
       "x26           -0.0678      0.083     -0.818      0.413      -0.230       0.095\n",
       "x27           -0.0394      0.078     -0.506      0.613      -0.192       0.113\n",
       "x28           -2.2808   1.04e+06   -2.2e-06      1.000   -2.03e+06    2.03e+06\n",
       "x29            0.1247      0.095      1.310      0.190      -0.062       0.311\n",
       "x30            0.1042      0.073      1.431      0.152      -0.038       0.247\n",
       "x31            0.0626      0.107      0.586      0.558      -0.147       0.272\n",
       "x32           -0.1430   2.15e+04  -6.64e-06      1.000   -4.22e+04    4.22e+04\n",
       "x33            0.0102      0.139      0.074      0.941      -0.261       0.282\n",
       "x34            0.0607      0.087      0.698      0.485      -0.110       0.231\n",
       "x35            0.0398      0.087      0.459      0.646      -0.130       0.210\n",
       "x36           -0.2354      0.114     -2.070      0.038      -0.458      -0.013\n",
       "x37            0.4981    339.695      0.001      0.999    -665.292     666.288\n",
       "x38            0.0512      0.092      0.556      0.578      -0.129       0.232\n",
       "x39            0.0431      0.082      0.526      0.599      -0.118       0.204\n",
       "x40            0.2837      0.147      1.934      0.053      -0.004       0.571\n",
       "x41            0.0669      0.070      0.951      0.341      -0.071       0.205\n",
       "x42           -0.9019   1.14e+05  -7.94e-06      1.000   -2.23e+05    2.23e+05\n",
       "x43            0.0169      0.073      0.231      0.818      -0.126       0.160\n",
       "x44           -0.1055      0.082     -1.284      0.199      -0.267       0.056\n",
       "x45           -0.0223      0.084     -0.265      0.791      -0.187       0.143\n",
       "x46           -6.7142    800.043     -0.008      0.993   -1574.770    1561.342\n",
       "x47           -0.0284      0.142     -0.200      0.842      -0.307       0.250\n",
       "x48            0.2534      0.237      1.068      0.286      -0.212       0.718\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.15 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd question regarding supervisors\n",
    "# p-value WITH DEMOGRAPHICS\n",
    "\n",
    "X_1 = independent_q1_alt\n",
    "Y_1 = dep_2.values.astype(int)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_1)\n",
    "X_transformed_1 = scaler.transform(X_1)\n",
    "\n",
    "X_transformed_1 = np.hstack([np.ones([X_transformed_1.shape[0],1]), X_transformed_1])\n",
    "\n",
    "logit = sm.Logit(Y_1, X_transformed_1)\n",
    "fitted_model_demo = logit.fit_regularized()\n",
    "fitted_model_demo.summary()\n",
    "\n",
    "# alpha = 0.05\n",
    "# all features together are significant\n",
    "# Columns that are significant: 2,3,4,5,9,15,19,20,21,22,23,35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model to get the feature importances\n",
    "\n",
    "# filter depreciation warning that is associated with not using \n",
    "# the most updated version of numpy and scikit-learnin the vitual environment\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "ind_q1 = independent_q1_alt.copy()\n",
    "ind_q1.columns = [np.arange(len(ind_q1.columns))]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ind_q1,dep_2.values.astype(int),test_size = 0.2)\n",
    "\n",
    "estimators = [(\"normalize\", preprocessing.StandardScaler()),\n",
    "             (\"model\",LogisticRegression())]\n",
    "\n",
    "pipe = pipeline.Pipeline(estimators)\n",
    "\n",
    "param_grid = [{\"model\":[XGBClassifier()], \n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__max_depth\":[1,2,3,4,5],\"model__n_estimators\":[50,100,150,200],\"model__n_jobs\":[6]},\n",
    "              {\"model\": [RandomForestClassifier()],\n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__n_estimators\":[100,150,200],\"model__n_jobs\":[6]}]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=6)\n",
    "fitted_grid_1 = grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7473347547974414"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_grid_1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['How willing would you be to share with friends and family that you have a mental illness?',\n",
       "       'Overall, how much importance does your employer place on mental health?',\n",
       "       'If a mental health issue prompted you to request a medical leave from work, how easy or difficult would it be to ask for that leave?__Very easy',\n",
       "       'If a mental health issue prompted you to request a medical leave from work, how easy or difficult would it be to ask for that leave?__Somewhat easy',\n",
       "       'Mood Disorder',\n",
       "       'If a mental health issue prompted you to request a medical leave from work, how easy or difficult would it be to ask for that leave?__Neither easy nor difficult',\n",
       "       'Have you ever sought treatment for a mental health disorder from a mental health professional?',\n",
       "       'Post-Traumatic Stress Disorder',\n",
       "       'Overall, how well do you think the tech industry supports employees with mental health issues?',\n",
       "       'Afrcian American',\n",
       "       'Does your employer offer resources to learn more about mental health disorders and options for seeking help?__Yes',\n",
       "       'Adjustment disorder'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ranking of factors that are important for predicting comfort level, from most to least important\n",
    "indices = np.flip(fitted_grid_1.best_estimator_.named_steps[\"model\"].feature_importances_.argsort())\n",
    "\n",
    "# creating a list of factors that are statistically significant\n",
    "sig_list = [2,3,4,5,9,15,19,20,21,22,23,35]\n",
    "\n",
    "numbers = []\n",
    "for position,idx in enumerate(indices):\n",
    "    for num in sig_list:\n",
    "        if idx == num:\n",
    "            numbers.append(idx)\n",
    "\n",
    "independent_q1_alt.columns[numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can companies to do encourage their employees to seek treatment?\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up 2 dataframes for concatenating data\n",
    "omitted = pd.DataFrame(columns = [\"Question\",\"Answer\"])\n",
    "final = pd.DataFrame()\n",
    "\n",
    "# current_mh_coverage\n",
    "for i in [0,2,3]:\n",
    "    result = make_dummies(current_mh_coverage[i],\"Yes\")\n",
    "    final = pd.concat([final,result],axis = 1)\n",
    "\n",
    "result = make_dummies_iloc(current_mh_coverage[5],[2,-2,-1])\n",
    "final = pd.concat([final,result],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a table of independend variables for this question\n",
    "ind_variable = pd.concat([final,demographics],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# determining statistical significance using statsmodel Logistic Regression\n",
    "dep_question = \"Have you ever sought treatment for a mental health disorder from a mental health professional?\"\n",
    "\n",
    "X_1 = ind_variable\n",
    "Y_1 = df_2.loc[:,dep_question]\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_1)\n",
    "X_transformed_1 = scaler.transform(X_1)\n",
    "\n",
    "X_transformed_1 = np.hstack([np.ones([X_transformed_1.shape[0],1]), X_transformed_1])\n",
    "\n",
    "logit = sm.Logit(Y_1, X_transformed_1)\n",
    "fitted_model_sp = logit.fit_regularized()\n",
    "fitted_model_sp.summary()\n",
    "\n",
    "# alpha = 0.05\n",
    "# all features together are significant\n",
    "# Columns that are significant: 0,5,6,7,8,12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model to get the feature importance\n",
    "ind_alt = ind_variable.copy()\n",
    "ind_alt.columns = [np.arange(len(ind_alt.columns))]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ind_alt,df_2.loc[:,dep_question],test_size = 0.2)\n",
    "\n",
    "estimators = [(\"normalize\", preprocessing.StandardScaler()),\n",
    "             (\"model\",LogisticRegression())]\n",
    "\n",
    "pipe = pipeline.Pipeline(estimators)\n",
    "\n",
    "param_grid = [{\"model\":[XGBClassifier()], \n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__max_depth\":[1,2,3,4,5],\"model__n_estimators\":[50,100,150,200],\"model__n_jobs\":[6]},\n",
    "              {\"model\": [RandomForestClassifier()],\n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__n_estimators\":[100,150,200],\"model__n_jobs\":[6]}]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "fitted_grid_2 = grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking of factors that are important for predicting comfort level, from most to least important\n",
    "indices = np.flip(fitted_grid_2.best_estimator_.named_steps[\"model\"].feature_importances_.argsort())\n",
    "\n",
    "# displaying features that have statistical significance\n",
    "sig_list = [0,5,6,7,8,12]\n",
    "\n",
    "numbers = []\n",
    "for position,idx in enumerate(indices):\n",
    "    for num in sig_list:\n",
    "        if idx == num:\n",
    "            numbers.append(idx)\n",
    "\n",
    "ind_variable.columns[numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Please continue to Capstone modelling stage v.3-RNN for the remainder of the project.\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
