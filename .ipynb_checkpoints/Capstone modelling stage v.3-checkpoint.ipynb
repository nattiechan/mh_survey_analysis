{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project\n",
    "-------\n",
    "\n",
    "### Stage 2 - Modelling phase\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages and data\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible packages that need to be installed:\n",
    "\n",
    "1. SpaCy\n",
    "\n",
    "<code> conda install -c spacy spacy </code>\n",
    "\n",
    "2. 'en_core_web_md' - library used in SpaCy\n",
    "\n",
    "<code> python -m spacy download en_core_web_md </code>\n",
    "\n",
    "3. wordcloud\n",
    "\n",
    "<code> conda install -c conda-forge wordcloud </code>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nattiechan/anaconda3/envs/myenv/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "\n",
    "# Basics\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import numpy as np\n",
    "\n",
    "# Graphs\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Preprocessing; model selection and evaluation\n",
    "from sklearn import pipeline, preprocessing\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# text handling\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# for custom countvectorizer with SpaCy lemmatization\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, VectorizerMixin\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# WordCloud\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "data = pd.read_csv(\"saved_csv/df.csv\")\n",
    "data.drop(columns = \"Unnamed: 0\",inplace=True)\n",
    "\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "------\n",
    "\n",
    "Below outlines the questions to be answered with the cleaned data, and proposed methodology for each question.\n",
    "\n",
    "<b><i><u>Question #1</b></i></u>\n",
    "\n",
    "**What are some of the qualitative insights for the tech industry to improve MH support for employees?**\n",
    "\n",
    "In the survey, there is a question asking participants to briefly describe what the industry can do as a whole to improve mental health (MH) support for their employees. The purpose of the analysis is to see whether there are any *underlying themes or patterns* to the responses.\n",
    "\n",
    "<i><u>Methodology</i></u>\n",
    "\n",
    "Process the text using *SpaCy Lemmatizer* and *CountVectorizer*, then visualize the patterns using WordCloud. Since the font on the WordCloud is based on the frequency of the words appearing in the entire dataset, the more frequently used words will be larger in the WordCloud, suggesting a pattern or theme.\n",
    "\n",
    "<b><i><u>Question #2</b></i></u>\n",
    "\n",
    "**What are the factors that affect comfort level in discussing MH at workplace?**\n",
    "\n",
    "One proposed hypothesis is that improving comfort level in discussing MH at workplace can contribute to improving MH support for employees in the tech industry. A *prescriptive analysis* will be done to determine the factors using various questions pertaining to employer's MH coverage, personal MH status and experience with having MH conversations at the workplace, and overall perceived ratings in the dataset as features.\n",
    "\n",
    "<i><u>Methodology</i></u>\n",
    "\n",
    "Determine the statistical significance of each feature and the model as a whole using *Statsmodels Logistic Regression*, then use either *Random Forrest Classifier* or *XGBoost Classifier* to determine feature importance.\n",
    "\n",
    "<b><i><u>Question #3</b></i></u>\n",
    "\n",
    "**Can we predict one's comfort level in discussing MH at workplace using participants' qualitative responses of ways to improve MH support?**\n",
    "\n",
    "A *predictive model* will be built to determine if the qualitative responses are viable predictors of one's comfort level in discussing MH at workplace.\n",
    "\n",
    "<i><u>Methodology</i></u>\n",
    "\n",
    "Design a model using either *Convolutional Neural Network (CNN)* or a *Recurrent Neural Network (RNN)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "------\n",
    "\n",
    "#### Independent Variables for Q2\n",
    "------\n",
    "\n",
    "Questions will be grouped into one of the following categories:\n",
    "\n",
    "- Current employer's MH coverage\n",
    "- Previous employer's MH coverage\n",
    "- MH status\n",
    "- Witnessed experience (of discussing MH in the workplace)\n",
    "- Overall perceived ratings\n",
    "- Comfort level discussing MH in the workplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting questions into categories\n",
    "\n",
    "current_mh_coverage = [\"Does your employer provide mental health benefits as part of healthcare coverage?\",\n",
    "               \"Do you know the options for mental health care available under your employer-provided health coverage?\",\n",
    "               \"Has your employer ever formally discussed mental health (for example, as part of a wellness campaign or other official communication)?\",\n",
    "               \"Does your employer offer resources to learn more about mental health disorders and options for seeking help?\",\n",
    "               \"Is your anonymity protected if you choose to take advantage of mental health or substance abuse treatment resources provided by your employer?\",\n",
    "               \"If a mental health issue prompted you to request a medical leave from work, how easy or difficult would it be to ask for that leave?\"]\n",
    "\n",
    "previous_mh_coverage = [\"Have your previous employers provided mental health benefits?\",\n",
    "                        \"Were you aware of the options for mental health care provided by your previous employers?\",\n",
    "                        \"Did your previous employers ever formally discuss mental health (as part of a wellness campaign or other official communication)?\",\n",
    "                        \"Did your previous employers provide resources to learn more about mental health disorders and how to seek help?\",\n",
    "                        \"Was your anonymity protected if you chose to take advantage of mental health or substance abuse treatment resources with previous employers?\"]\n",
    "\n",
    "mh_status = [\"Do you currently have a mental health disorder?\",\n",
    "             \"Have you ever been diagnosed with a mental health disorder?\",'Anxiety Disorder', 'Mood Disorder', \n",
    "             'Psychotic Disorder','Eating Disorder', 'Neurodevelopmental Disorders','Personality Disorder', \n",
    "             'Obsessive-Compulsive Disorder','Post-Traumatic Stress Disorder', 'Dissociative Disorder',\n",
    "             'Substance-Related and Addictive Disorders', 'Other','Adjustment disorder',\n",
    "             \"Have you had a mental health disorder in the past?\",\n",
    "             \"Have you ever sought treatment for a mental health disorder from a mental health professional?\",\n",
    "             \"Do you have a family history of mental illness?\",\n",
    "             \"How willing would you be to share with friends and family that you have a mental illness?\",\n",
    "             \"Would you be willing to bring up a physical health issue with a potential employer in an interview?\"]\n",
    "\n",
    "witnessed_exp = [\"Have your observations of how another individual who discussed a mental health issue made you less likely to reveal a mental health issue yourself in your current workplace?\",\n",
    "                 \"Have you observed or experienced an unsupportive or badly handled response to a mental health issue in your current or previous workplace?\",\n",
    "                 \"Have you observed or experienced supportive or well handled response to a mental health issue in your current or previous workplace?\"]\n",
    "\n",
    "ratings = df.columns[df.columns.str.contains(\"Overall\")]\n",
    "\n",
    "demographics = [\"What is your age?\",\"What is your gender?\",\"What country do you live in?\",\n",
    "                \"What US state or territory do you live in?\",\"What is your race?\"]\n",
    "\n",
    "comfort_talking_current = [\"Would you feel comfortable discussing a mental health issue with your direct supervisor(s)?\",\n",
    "                           \"Have you ever discussed your mental health with your employer?\",\n",
    "                           \"Would you feel comfortable discussing a mental health issue with your coworkers?\",\n",
    "                           \"Have you ever discussed your mental health with coworkers?\",\n",
    "                           \"Have you ever had a coworker discuss their or another coworker's mental health with you?\",\n",
    "                           \"Would you feel more comfortable talking to your coworkers about your physical health or your mental health?\",\n",
    "                           \"Would you bring up your mental health with a potential employer in an interview?\",\n",
    "                           \"Are you openly identified at work as a person with a mental health issue?\"]\n",
    "\n",
    "comfort_talking_previous = [\"Would you have felt more comfortable talking to your previous employer about your physical health or your mental health?\",\n",
    "                            \"Would you have been willing to discuss your mental health with your direct supervisor(s)?\",\n",
    "                            \"Did you ever discuss your mental health with your previous employer?\",\n",
    "                            \"Would you have been willing to discuss your mental health with your coworkers at previous employers?\",\n",
    "                            \"Did you ever discuss your mental health with a previous coworker(s)?\",\n",
    "                            \"Did you ever have a previous coworker discuss their or another coworker's mental health with you?\",\n",
    "                            \"Would you bring up your mental health with a potential employer in an interview?\",\n",
    "                            \"Are you openly identified at work as a person with a mental health issue?\"]\n",
    "\n",
    "comfort_dependent_var = [\"Would you feel comfortable discussing a mental health issue with your direct supervisor(s)?\",\n",
    "                           \"Have you ever discussed your mental health with your employer?\",\n",
    "                           \"Would you feel comfortable discussing a mental health issue with your coworkers?\",\n",
    "                           \"Have you ever discussed your mental health with coworkers?\",\n",
    "                           \"Have you ever had a coworker discuss their or another coworker's mental health with you?\",\n",
    "                           \"Would you feel more comfortable talking to your coworkers about your physical health or your mental health?\",\n",
    "                           \"Would you have felt more comfortable talking to your previous employer about your physical health or your mental health?\",\n",
    "                           \"Would you have been willing to discuss your mental health with your direct supervisor(s)?\",\n",
    "                           \"Did you ever discuss your mental health with your previous employer?\",\n",
    "                           \"Would you have been willing to discuss your mental health with your coworkers at previous employers?\",\n",
    "                           \"Did you ever discuss your mental health with a previous coworker(s)?\",\n",
    "                           \"Did you ever have a previous coworker discuss their or another coworker's mental health with you?\",\n",
    "                           \"Would you bring up your mental health with a potential employer in an interview?\",\n",
    "                           \"Are you openly identified at work as a person with a mental health issue?\"]\n",
    "\n",
    "categories = [current_mh_coverage,previous_mh_coverage,mh_status,witnessed_exp,\n",
    "              ratings,comfort_talking_current,comfort_talking_previous,comfort_dependent_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various questions from current employer's MH coverage, MH status, witnessed experience as well as overall perceived ratings will be chosen as indpendent variables.\n",
    "\n",
    "Dummy variables will be generated from the responses of each question. Responses will be grouped in two general categories (eg. Yes/No) and for the independent variables, one group of responses will be dropped to avoid multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating functions for dummy generation\n",
    "def make_dummies(question,columns_to_keep = \"Yes\"):\n",
    "    '''\n",
    "    The function creates dummy variables for independent variables.\n",
    "    You can specify the column to KEEP.\n",
    "    \n",
    "    Inputs:\n",
    "    ------\n",
    "    question: the question/column name that you wish to create a dummy vairable for\n",
    "    columns_to_keep: a string indicating the answer you want to keep\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    result: a DataFrame of the desire column in dummy variables.\n",
    "    '''\n",
    "    \n",
    "    dummies = pd.get_dummies(df_2.loc[:,question])\n",
    "    for j in range(len(dummies.columns)):\n",
    "        name = question + \"__\" + columns_to_keep\n",
    "        dummies.rename(columns = {columns_to_keep : name},inplace=True)\n",
    "\n",
    "    result = dummies.loc[:,[name]]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def make_dummies_iloc(question,list_to_keep):\n",
    "    '''\n",
    "    The function creates dummy variables for independent variables.\n",
    "    This is designed for instances when you need to keep multiple responses.\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    question: the question/column name that you wish to create a dummy variable for\n",
    "    list_to_keep: a list of numbers corresponding the column numbers of the responses \n",
    "                  (after it has been converted to dummy variables) that you want to keep\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    result: A DataFrame of the desire column in dummy variables.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    dummies = pd.get_dummies(df_2.loc[:,question])\n",
    "    for j in range(len(dummies.columns)):\n",
    "        name = question + \"__\" + dummies.columns[j]\n",
    "        dummies.rename(columns = {dummies.columns[j] : name},inplace=True)\n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    for num in list_to_keep:\n",
    "        to_keep = dummies.iloc[:,num]\n",
    "        result = pd.concat([result,to_keep],axis=1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a copy of the data\n",
    "df_2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up 2 dataframes for concatenating data\n",
    "omitted = pd.DataFrame(columns = [\"Question\",\"Answer\"])\n",
    "final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# current_mh_coverage\n",
    "for i in [0,2,3]:\n",
    "    result = make_dummies(current_mh_coverage[i],\"Yes\")\n",
    "    final = pd.concat([final,result],axis = 1)\n",
    "\n",
    "result = make_dummies_iloc(current_mh_coverage[5],[2,-2,-1])\n",
    "final = pd.concat([final,result],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# witnessed_exp\n",
    "result = make_dummies(witnessed_exp[0],\"Yes\")\n",
    "final = pd.concat([final,result],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mh_status\n",
    "\n",
    "# Modifying some responses for ease of sorting\n",
    "old_answer_1 = \"Possibly\"\n",
    "old_answer_2 = \"-1\"\n",
    "answer = \"Don't Know\"\n",
    "\n",
    "to_dummy = []\n",
    "for i in [0,1,-5,-3,-1]:\n",
    "    to_dummy.append(mh_status[i])\n",
    "\n",
    "for num in [0,2]:\n",
    "    df_2.loc[:,to_dummy[num]][df_2.loc[:,to_dummy[num]]==old_answer_1]=answer\n",
    "\n",
    "df_2.loc[:,to_dummy[2]][df_2.loc[:,to_dummy[2]]==old_answer_2]=answer\n",
    "\n",
    "#creating dummy variables\n",
    "result = make_dummies(to_dummy[0],\"Yes\")\n",
    "final = pd.concat([final,result],axis = 1)\n",
    "\n",
    "for i in [2,3,4,5,6,7,8,9,10,11,12,13,15,17]:\n",
    "    final = pd.concat([final,df_2.loc[:,mh_status[i]]],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings\n",
    "for i in [1,4]:\n",
    "    final = pd.concat([final,df_2.loc[:,ratings[i]]],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizing the table of independent variables\n",
    "independent_q1 = final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependent Variables for Q2\n",
    "------\n",
    "\n",
    "Various questions from questions from comfort level of discussing MH in the workplace will be chosen as the dependent variable.\n",
    "\n",
    "Dummy variables will be generated from the responses of each question. Clusters of classes will be created afterwards with unsupervised clustering methods using the dummy variables as independent variables. In contrast to the independent variable, all responses can be kept since there is no issue of multicollinearity with dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a table of dependent variables after turning the responses into dummy variables\n",
    "dependent = df.loc[:,comfort_dependent_var]\n",
    "\n",
    "num_list = [0,2,5,6,7,9,12]\n",
    "columns_to_join = [1,3,4,8,10,11,13]\n",
    "\n",
    "final_dep = pd.get_dummies(dependent.iloc[:,num_list])\n",
    "\n",
    "for i in columns_to_join:\n",
    "    result = pd.get_dummies(dependent.iloc[:,i])\n",
    "    final_dep = pd.concat([final_dep,result],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting classes\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans Clustering will be used to generate the clusters. The goal is to generate 4 clusters with a relatively normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KMeans Clustering\n",
    "X = final_dep\n",
    "\n",
    "inertia = []\n",
    "\n",
    "for num in range(1,21):\n",
    "    k_means_model = KMeans(n_clusters = num)\n",
    "    k_means_model.fit(X)\n",
    "\n",
    "    y_pred = k_means_model.predict(X)\n",
    "    inertia.append(k_means_model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(1,21),inertia)\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.xticks(np.arange(0,21,2))\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Inertia of KMeans model by number of clusters\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inertia is a metric that can be used to determine the optimal number of clusters for KMeans Clustering. As we can see with the graph illustrating the inertia at different number of clusters, 4 clusters would be an appropriate choice to avoid overfitting since the curve is starting to flatten out at around that mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2e14b7bcf16f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Predicting the classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mk_means_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mk_means_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_means_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Predicting the classes\n",
    "k_means_model = KMeans(n_clusters = 4)\n",
    "k_means_model.fit(X)\n",
    "\n",
    "y_pred = k_means_model.predict(X)\n",
    "\n",
    "# Visualizing the results\n",
    "clusters = pd.DataFrame(y_pred)\n",
    "clusters.rename(columns = {0 : \"Class\"},inplace=True)\n",
    "clusters[\"Class\"].groupby(clusters[\"Class\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annotate the data points with the KMeans prediction\n",
    "Y = k_means_model.predict(X)\n",
    "\n",
    "#Plot\n",
    "plt.scatter(X.iloc[:, 5], X.iloc[:, 6], c=Y, edgecolor='k')\n",
    "\n",
    "#Plot the K centers\n",
    "plt.scatter(k_means_model.cluster_centers_[:, 0],k_means_model.cluster_centers_[:, 1], c='red',marker=\"*\",s=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cluster 0: \n",
      "Would you feel comfortable discussing a mental health issue with your direct supervisor(s)?_No\n",
      "Would you feel comfortable discussing a mental health issue with your coworkers?_No\n",
      "Would you feel more comfortable talking to your coworkers about your physical health or your mental health?_Physical health\n",
      "Would you have felt more comfortable talking to your previous employer about your physical health or your mental health?_Physical health\n",
      "Would you have been willing to discuss your mental health with your direct supervisor(s)?_No, none of my previous supervisors\n",
      "Would you have been willing to discuss your mental health with your coworkers at previous employers?_No, at none of my previous employers\n",
      "Would you bring up your mental health with a potential employer in an interview?_No\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "cluster 1: \n",
      "Would you feel comfortable discussing a mental health issue with your direct supervisor(s)?_Not Applicable\n",
      "Would you feel comfortable discussing a mental health issue with your coworkers?_Not Applicable\n",
      "Would you feel more comfortable talking to your coworkers about your physical health or your mental health?_Not Applicable\n",
      "Would you have felt more comfortable talking to your previous employer about your physical health or your mental health?_Physical health\n",
      "Would you have been willing to discuss your mental health with your coworkers at previous employers?_Some of my previous employers\n",
      "Would you bring up your mental health with a potential employer in an interview?_No\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "cluster 2: \n",
      "Would you feel comfortable discussing a mental health issue with your direct supervisor(s)?_Yes\n",
      "Would you feel comfortable discussing a mental health issue with your coworkers?_Yes\n",
      "Would you feel more comfortable talking to your coworkers about your physical health or your mental health?_Same level of comfort for each\n",
      "Would you have felt more comfortable talking to your previous employer about your physical health or your mental health?_Physical health\n",
      "Would you have been willing to discuss your mental health with your direct supervisor(s)?_Some of my previous supervisors\n",
      "Would you have been willing to discuss your mental health with your coworkers at previous employers?_Some of my previous employers\n",
      "Would you bring up your mental health with a potential employer in an interview?_No\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "cluster 3: \n",
      "Would you feel comfortable discussing a mental health issue with your direct supervisor(s)?_Maybe\n",
      "Would you feel comfortable discussing a mental health issue with your coworkers?_Maybe\n",
      "Would you feel more comfortable talking to your coworkers about your physical health or your mental health?_Physical health\n",
      "Would you have felt more comfortable talking to your previous employer about your physical health or your mental health?_Physical health\n",
      "Would you have been willing to discuss your mental health with your coworkers at previous employers?_Some of my previous employers\n",
      "Would you bring up your mental health with a potential employer in an interview?_No\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Method source: https://medium.com/@davidmasse8/unsupervised-learning-for-categorical-data-dd7e497033ae\n",
    "# Code source: https://github.com/nicodv/kmodes\n",
    "from kmodes.kmodes import KModes\n",
    "# define the k-modes model\n",
    "km = KModes(n_clusters=4, init='Huang', n_init=5, verbose=0)\n",
    "\n",
    "X = final_dep\n",
    "# # fit the clusters to the skills dataframe\n",
    "clusters = km.fit_predict(X)\n",
    "# get an array of cluster modes\n",
    "kmodes = km.cluster_centroids_\n",
    "shape = kmodes.shape\n",
    "# For each cluster mode (a vector of \"1\" and \"0\")\n",
    "# find and print the column headings where \"1\" appears.\n",
    "# If no \"1\" appears, assign to \"no-skills\" cluster.\n",
    "for i in range(shape[0]):\n",
    "    if sum(kmodes[i,:]) == 0:\n",
    "        print(\"\\ncluster \" + str(i) + \": \")\n",
    "        print(\"no-skills cluster\")\n",
    "    else:\n",
    "        print(\"\\ncluster \" + str(i) + \": \")\n",
    "        cent = kmodes[i,:]\n",
    "        for j in final_dep.columns[np.nonzero(cent)]:\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the results\n",
    "table = pd.DataFrame(clusters)\n",
    "table.rename(columns = {0 : \"Class\"},inplace=True)\n",
    "table[\"Class\"].groupby(table[\"Class\"]).count()\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table.to_csv(\"saved_csv/q1_dependent_alt.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is an element of randomness in KMeans Clustering, the code will generate different counts with each iteration. Since the goal is to generate a cluster of 4 classes with an approximately normal distribution, the following cluster count will be used for the analysis:\n",
    "\n",
    "|Class|Count|\n",
    "|------|------|\n",
    "|0|143|\n",
    "|1|490|\n",
    "|2|386|\n",
    "|3|154|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### What are some of the qualitative insights for the tech industry to improve MH support for employees?\n",
    "-----\n",
    "\n",
    "Qualitative responses from the question \"briefly describe what you think the industry as a whole and/or employers could do to improve mental health support for employees\" will be processed using [spacy-vectorizers](https://github.com/mpavlovic/spacy-vectorizers), a custom CountVectorizer with SpaCy lemmatization embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using spacy-vectorizers to process texts\n",
    "# Code source: https://github.com/mpavlovic/spacy-vectorizers\n",
    "\n",
    "class SpacyPipeInitializer(object):\n",
    "    def __init__(self, nlp, join_str=\" \", batch_size=10000, n_threads=2):\n",
    "        self.nlp = nlp\n",
    "        self.join_str = join_str\n",
    "        self.batch_size = batch_size\n",
    "        self.n_threads = n_threads\n",
    "        \n",
    "class SpacyPipeProcessor(SpacyPipeInitializer):\n",
    "    def __init__(self, nlp, multi_iters=False, join_str=\" \", batch_size=10000, n_threads=2):\n",
    "        super(SpacyPipeProcessor, self).__init__(nlp, join_str, batch_size, n_threads)\n",
    "        self.multi_iters = multi_iters\n",
    "    \n",
    "    def __call__(self, raw_documents):\n",
    "        docs_generator = self.nlp.pipe(raw_documents, batch_size=self.batch_size, n_threads=self.n_threads)\n",
    "        return docs_generator if self.multi_iters == False else list(docs_generator)\n",
    "    \n",
    "class SpacyLemmaCountVectorizer(CountVectorizer):\n",
    "    \n",
    "    def __init__(self, input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None,\n",
    "                 lowercase=True, preprocessor=None, tokenizer=None,\n",
    "                 stop_words=None, token_pattern=r\"(?u)[^\\r\\n ]+\",\n",
    "                 ngram_range=(1, 1), analyzer='word',\n",
    "                 max_df=1.0, min_df=1, max_features=None,\n",
    "                 vocabulary=None, binary=False, dtype=np.int64, \n",
    "                 nlp=None, ignore_chars='!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~', \n",
    "                 join_str=\" \", use_pron=False):\n",
    "        \n",
    "        super().__init__(input, encoding, decode_error, strip_accents, \n",
    "                                                   lowercase, preprocessor, tokenizer,\n",
    "                                                   stop_words, token_pattern, ngram_range, \n",
    "                                                   analyzer, max_df, min_df, max_features,\n",
    "                                                   vocabulary, binary, dtype)\n",
    "        self.ignore_chars = ignore_chars\n",
    "        self.join_str = ' ' # lemmas have to be joined for splitting\n",
    "        self.use_pron = use_pron\n",
    "        self.translate_table = dict((ord(char), None) for char in self.ignore_chars)\n",
    "        \n",
    "    def lemmatize_from_docs(self, docs):\n",
    "        for doc in docs:\n",
    "            lemmas_gen = (token.lemma_.translate(self.translate_table) if self.use_pron or token.lemma_!='-PRON-' else token.lower_.translate(self.translate_table) for token in doc)  # generator expression\n",
    "            yield self.join_str.join(lemmas_gen) if self.join_str is not None else [lemma for lemma in lemmas_gen]\n",
    "    \n",
    "    def build_tokenizer(self):\n",
    "        return lambda doc: doc.split()\n",
    "    \n",
    "    def transform(self, spacy_docs):\n",
    "        raw_documents = self.lemmatize_from_docs(spacy_docs)\n",
    "        return super(SpacyLemmaCountVectorizer, self).transform(raw_documents)\n",
    "    \n",
    "    def fit_transform(self, spacy_docs, y=None):\n",
    "        raw_documents = self.lemmatize_from_docs(spacy_docs)\n",
    "        return super(SpacyLemmaCountVectorizer, self).fit_transform(raw_documents, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grabbing the text responses\n",
    "corpus = df.iloc[:,-9]\n",
    "\n",
    "# customization stopwords to filter out some words\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"mental\",\"health\",\"issue\",\"work\",\n",
    "                  \"take\",\"hour\",\"tech\",\"industry\",\"people\",\"employee\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer with SpaCy Lemmatization\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "spp = SpacyPipeProcessor(nlp, n_threads=1, multi_iters=True)\n",
    "spacy_docs = spp(corpus);\n",
    "\n",
    "slcv = SpacyLemmaCountVectorizer(min_df=3,stop_words=stopwords, ngram_range=(1, 3), ignore_chars='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "slcv.fit(spacy_docs)\n",
    "count_vectors = slcv.transform(spacy_docs); count_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the text has been processed, a WordCloud is generated to visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pulling out the list of parsed words and put them into a wordcloud\n",
    "list_of_words = slcv.vocabulary_.keys()\n",
    "list_of_words = list(list_of_words)\n",
    "list_of_words.sort()\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(\" \".join(list_of_words))\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words with bigger fonts in the WordCloud are ones that occur in the dataset with higher frequency. From the WordCloud above, it seems like words like \"open\", \"support\", \"talk\" are big themes of the responses. This finding supports the need of looking deeper into factos that affect comfort level in discussing MH in the workplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #2\n",
    "\n",
    "### What are the factors that affect comfort level in discussing MH at workplace?\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that creates dummy variables\n",
    "def make_dummies_q(column_num,column_name):\n",
    "    '''\n",
    "    This function creates a dummy variable for a particular column,\n",
    "    and drops a column to avoid multicollinearity since the responses will be included in the independent variables.\n",
    "    \n",
    "    Inputs:\n",
    "    ------\n",
    "    column_num = an integer of the number of column/question you wish to create a dummy variable for\n",
    "    column_name = the response that you wish to drop to avoid multicollinearity\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    result = a DataFrame of dummy variables, containing n-1 answers\n",
    "    '''\n",
    "\n",
    "    result = pd.get_dummies(df_2.iloc[:,column_num])\n",
    "    result.drop(columns = column_name, inplace=True)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing the dependent variables (after they have been clustered)\n",
    "dependent_class = pd.read_csv(\"saved_csv/q1_dependent_alt.csv\")\n",
    "dependent_class.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplifing dependent variables to 0/1\n",
    "dependent_class_alt = dependent_class.copy()\n",
    "\n",
    "dependent_class_alt[dependent_class_alt < 2] = 0\n",
    "dependent_class_alt[dependent_class_alt >= 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What is your gender?\n",
    "gender = make_dummies_q(-7,\"Male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping countries into 5 continents\n",
    "country_names = df_2.iloc[:,-6].groupby(df_2.iloc[:,-6]).count().index\n",
    "\n",
    "df_2[\"countries_continent\"] = df_2.iloc[:,-6]\n",
    "\n",
    "north_am = [8,34,55]\n",
    "south_am = [0,6,9,56]\n",
    "asia = [3,20,23,24,26,28,29,39,43,44,46]\n",
    "africa = [14,30,33,37,48]\n",
    "europe = [2,4,5,7,10,11,13,15,16,17,18,19,21,22,25,27,31,32,35,38,40,41,42,45,47,49,50,51,52,53,54]\n",
    "oceania = [1,36]\n",
    "did_not_answer = [12]\n",
    "\n",
    "continents = [north_am,south_am,asia,africa,europe,oceania,did_not_answer]\n",
    "names = [\"North America\", \"South America\", \"Asia\", \"Africa\", \"Europe\", \"Oceania\", \"Did not answer\"]\n",
    "\n",
    "for position,continent in enumerate(continents):\n",
    "    for num in np.flip(continent):\n",
    "        df_2.loc[:,\"countries_continent\"][df_2.loc[:,\"countries_continent\"]== country_names[num]]=names[position]\n",
    "        \n",
    "countries = make_dummies_q(-1,\"North America\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is your race?\n",
    "races = make_dummies_q(-5,\"Caucasian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_size\n",
    "company_size = make_dummies_q(2,\"0\")\n",
    "#company_size.drop(columns = [company_size.columns[0],company_size.columns[-2]],inplace=True)\n",
    "\n",
    "# select companies with larger size into the independent variables\n",
    "size = company_size.iloc[:,[3,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a table of demographics\n",
    "demographics = pd.concat([gender,countries,races,size,df_2.iloc[:,[-9,3,4]]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating features/independent variables\n",
    "independent_q1_alt = pd.concat([independent_q1,demographics],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.46699088919801174\n",
      "            Iterations: 90\n",
      "            Function evaluations: 91\n",
      "            Gradient evaluations: 90\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Class</td>      <th>  No. Observations:  </th>  <td>  1173</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1121</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>    51</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Mon, 25 Mar 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.2741</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>10:22:12</td>     <th>  Log-Likelihood:    </th> <td> -547.78</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -754.67</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>2.805e-58</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   -1.2224</td> <td>  667.151</td> <td>   -0.002</td> <td> 0.999</td> <td>-1308.814</td> <td> 1306.370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.2158</td> <td>    0.092</td> <td>    2.337</td> <td> 0.019</td> <td>    0.035</td> <td>    0.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.1287</td> <td>    0.089</td> <td>    1.444</td> <td> 0.149</td> <td>   -0.046</td> <td>    0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0513</td> <td>    0.091</td> <td>    0.560</td> <td> 0.575</td> <td>   -0.128</td> <td>    0.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0903</td> <td>    0.078</td> <td>    1.152</td> <td> 0.249</td> <td>   -0.063</td> <td>    0.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.1830</td> <td>    0.085</td> <td>    2.163</td> <td> 0.031</td> <td>    0.017</td> <td>    0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.2434</td> <td>    0.087</td> <td>    2.802</td> <td> 0.005</td> <td>    0.073</td> <td>    0.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -0.0563</td> <td>    0.080</td> <td>   -0.704</td> <td> 0.481</td> <td>   -0.213</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    0.2029</td> <td>    0.106</td> <td>    1.920</td> <td> 0.055</td> <td>   -0.004</td> <td>    0.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.1121</td> <td>    0.088</td> <td>    1.278</td> <td> 0.201</td> <td>   -0.060</td> <td>    0.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>    0.0389</td> <td>    0.094</td> <td>    0.416</td> <td> 0.678</td> <td>   -0.144</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>   -0.0068</td> <td>    0.070</td> <td>   -0.097</td> <td> 0.923</td> <td>   -0.144</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>    0.0326</td> <td>    0.076</td> <td>    0.428</td> <td> 0.669</td> <td>   -0.117</td> <td>    0.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>    0.0041</td> <td>    0.078</td> <td>    0.052</td> <td> 0.958</td> <td>   -0.149</td> <td>    0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>   -0.0536</td> <td>    0.075</td> <td>   -0.715</td> <td> 0.475</td> <td>   -0.201</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   -0.1152</td> <td>    0.077</td> <td>   -1.504</td> <td> 0.133</td> <td>   -0.265</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>    0.0919</td> <td>    0.078</td> <td>    1.172</td> <td> 0.241</td> <td>   -0.062</td> <td>    0.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>    0.1151</td> <td>    0.074</td> <td>    1.550</td> <td> 0.121</td> <td>   -0.030</td> <td>    0.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>    0.0495</td> <td>    0.076</td> <td>    0.652</td> <td> 0.515</td> <td>   -0.099</td> <td>    0.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.2141</td> <td>    0.121</td> <td>    1.768</td> <td> 0.077</td> <td>   -0.023</td> <td>    0.451</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>    0.0426</td> <td>    0.080</td> <td>    0.535</td> <td> 0.593</td> <td>   -0.114</td> <td>    0.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>    0.3791</td> <td>    0.103</td> <td>    3.672</td> <td> 0.000</td> <td>    0.177</td> <td>    0.581</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    0.5273</td> <td>    0.091</td> <td>    5.781</td> <td> 0.000</td> <td>    0.349</td> <td>    0.706</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>    0.1647</td> <td>    0.087</td> <td>    1.891</td> <td> 0.059</td> <td>   -0.006</td> <td>    0.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    0.0557</td> <td>    0.085</td> <td>    0.653</td> <td> 0.514</td> <td>   -0.112</td> <td>    0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>    0.0786</td> <td>    0.103</td> <td>    0.766</td> <td> 0.444</td> <td>   -0.122</td> <td>    0.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>    0.0253</td> <td>    0.081</td> <td>    0.312</td> <td> 0.755</td> <td>   -0.134</td> <td>    0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>   -0.0676</td> <td>    0.074</td> <td>   -0.914</td> <td> 0.361</td> <td>   -0.213</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   -0.0859</td> <td>    0.079</td> <td>   -1.085</td> <td> 0.278</td> <td>   -0.241</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>   -0.0516</td> <td>    0.075</td> <td>   -0.692</td> <td> 0.489</td> <td>   -0.198</td> <td>    0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>    0.0373</td> <td>    0.080</td> <td>    0.467</td> <td> 0.641</td> <td>   -0.119</td> <td>    0.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>    0.0280</td> <td>    0.126</td> <td>    0.222</td> <td> 0.825</td> <td>   -0.220</td> <td>    0.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>   -0.4145</td> <td>  181.440</td> <td>   -0.002</td> <td> 0.998</td> <td> -356.031</td> <td>  355.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    0.2433</td> <td>    0.169</td> <td>    1.442</td> <td> 0.149</td> <td>   -0.087</td> <td>    0.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>    0.2720</td> <td>    0.094</td> <td>    2.894</td> <td> 0.004</td> <td>    0.088</td> <td>    0.456</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>    0.1582</td> <td>    0.088</td> <td>    1.791</td> <td> 0.073</td> <td>   -0.015</td> <td>    0.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   -0.0789</td> <td>    0.069</td> <td>   -1.144</td> <td> 0.253</td> <td>   -0.214</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>    0.6467</td> <td> 1832.505</td> <td>    0.000</td> <td> 1.000</td> <td>-3590.997</td> <td> 3592.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>    0.0589</td> <td>    0.077</td> <td>    0.765</td> <td> 0.444</td> <td>   -0.092</td> <td>    0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>   -1.0281</td> <td> 1.09e+04</td> <td> -9.4e-05</td> <td> 1.000</td> <td>-2.14e+04</td> <td> 2.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>   -0.3242</td> <td>    0.179</td> <td>   -1.816</td> <td> 0.069</td> <td>   -0.674</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>   -0.1325</td> <td>    0.080</td> <td>   -1.656</td> <td> 0.098</td> <td>   -0.289</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>   -0.7135</td> <td> 4991.425</td> <td>   -0.000</td> <td> 1.000</td> <td>-9783.726</td> <td> 9782.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>   -1.0527</td> <td> 1.06e+04</td> <td>-9.93e-05</td> <td> 1.000</td> <td>-2.08e+04</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>   -0.6556</td> <td> 5388.219</td> <td>   -0.000</td> <td> 1.000</td> <td>-1.06e+04</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>   -0.0045</td> <td>    0.071</td> <td>   -0.063</td> <td> 0.950</td> <td>   -0.144</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>   -0.3255</td> <td>   71.807</td> <td>   -0.005</td> <td> 0.996</td> <td> -141.064</td> <td>  140.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>    0.0494</td> <td>    0.073</td> <td>    0.680</td> <td> 0.496</td> <td>   -0.093</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>   -0.0955</td> <td>    0.082</td> <td>   -1.169</td> <td> 0.242</td> <td>   -0.256</td> <td>    0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>    0.0656</td> <td>    0.082</td> <td>    0.802</td> <td> 0.423</td> <td>   -0.095</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>    0.5610</td> <td>    0.133</td> <td>    4.207</td> <td> 0.000</td> <td>    0.300</td> <td>    0.822</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>    0.6655</td> <td>    0.174</td> <td>    3.815</td> <td> 0.000</td> <td>    0.324</td> <td>    1.007</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:                 1173\n",
       "Model:                          Logit   Df Residuals:                     1121\n",
       "Method:                           MLE   Df Model:                           51\n",
       "Date:                Mon, 25 Mar 2019   Pseudo R-squ.:                  0.2741\n",
       "Time:                        10:22:12   Log-Likelihood:                -547.78\n",
       "converged:                       True   LL-Null:                       -754.67\n",
       "                                        LLR p-value:                 2.805e-58\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -1.2224    667.151     -0.002      0.999   -1308.814    1306.370\n",
       "x1             0.2158      0.092      2.337      0.019       0.035       0.397\n",
       "x2             0.1287      0.089      1.444      0.149      -0.046       0.303\n",
       "x3             0.0513      0.091      0.560      0.575      -0.128       0.231\n",
       "x4             0.0903      0.078      1.152      0.249      -0.063       0.244\n",
       "x5             0.1830      0.085      2.163      0.031       0.017       0.349\n",
       "x6             0.2434      0.087      2.802      0.005       0.073       0.414\n",
       "x7            -0.0563      0.080     -0.704      0.481      -0.213       0.100\n",
       "x8             0.2029      0.106      1.920      0.055      -0.004       0.410\n",
       "x9             0.1121      0.088      1.278      0.201      -0.060       0.284\n",
       "x10            0.0389      0.094      0.416      0.678      -0.144       0.222\n",
       "x11           -0.0068      0.070     -0.097      0.923      -0.144       0.130\n",
       "x12            0.0326      0.076      0.428      0.669      -0.117       0.182\n",
       "x13            0.0041      0.078      0.052      0.958      -0.149       0.157\n",
       "x14           -0.0536      0.075     -0.715      0.475      -0.201       0.093\n",
       "x15           -0.1152      0.077     -1.504      0.133      -0.265       0.035\n",
       "x16            0.0919      0.078      1.172      0.241      -0.062       0.246\n",
       "x17            0.1151      0.074      1.550      0.121      -0.030       0.261\n",
       "x18            0.0495      0.076      0.652      0.515      -0.099       0.199\n",
       "x19            0.2141      0.121      1.768      0.077      -0.023       0.451\n",
       "x20            0.0426      0.080      0.535      0.593      -0.114       0.199\n",
       "x21            0.3791      0.103      3.672      0.000       0.177       0.581\n",
       "x22            0.5273      0.091      5.781      0.000       0.349       0.706\n",
       "x23            0.1647      0.087      1.891      0.059      -0.006       0.335\n",
       "x24            0.0557      0.085      0.653      0.514      -0.112       0.223\n",
       "x25            0.0786      0.103      0.766      0.444      -0.122       0.280\n",
       "x26            0.0253      0.081      0.312      0.755      -0.134       0.184\n",
       "x27           -0.0676      0.074     -0.914      0.361      -0.213       0.077\n",
       "x28           -0.0859      0.079     -1.085      0.278      -0.241       0.069\n",
       "x29           -0.0516      0.075     -0.692      0.489      -0.198       0.095\n",
       "x30            0.0373      0.080      0.467      0.641      -0.119       0.194\n",
       "x31            0.0280      0.126      0.222      0.825      -0.220       0.276\n",
       "x32           -0.4145    181.440     -0.002      0.998    -356.031     355.202\n",
       "x33            0.2433      0.169      1.442      0.149      -0.087       0.574\n",
       "x34            0.2720      0.094      2.894      0.004       0.088       0.456\n",
       "x35            0.1582      0.088      1.791      0.073      -0.015       0.331\n",
       "x36           -0.0789      0.069     -1.144      0.253      -0.214       0.056\n",
       "x37            0.6467   1832.505      0.000      1.000   -3590.997    3592.290\n",
       "x38            0.0589      0.077      0.765      0.444      -0.092       0.210\n",
       "x39           -1.0281   1.09e+04   -9.4e-05      1.000   -2.14e+04    2.14e+04\n",
       "x40           -0.3242      0.179     -1.816      0.069      -0.674       0.026\n",
       "x41           -0.1325      0.080     -1.656      0.098      -0.289       0.024\n",
       "x42           -0.7135   4991.425     -0.000      1.000   -9783.726    9782.299\n",
       "x43           -1.0527   1.06e+04  -9.93e-05      1.000   -2.08e+04    2.08e+04\n",
       "x44           -0.6556   5388.219     -0.000      1.000   -1.06e+04    1.06e+04\n",
       "x45           -0.0045      0.071     -0.063      0.950      -0.144       0.135\n",
       "x46           -0.3255     71.807     -0.005      0.996    -141.064     140.413\n",
       "x47            0.0494      0.073      0.680      0.496      -0.093       0.192\n",
       "x48           -0.0955      0.082     -1.169      0.242      -0.256       0.065\n",
       "x49            0.0656      0.082      0.802      0.423      -0.095       0.226\n",
       "x50            0.5610      0.133      4.207      0.000       0.300       0.822\n",
       "x51            0.6655      0.174      3.815      0.000       0.324       1.007\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p-value WITH DEMOGRAPHICS\n",
    "\n",
    "X_1 = independent_q1_alt\n",
    "Y_1 = dependent_class_alt\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_1)\n",
    "X_transformed_1 = scaler.transform(X_1)\n",
    "\n",
    "X_transformed_1 = np.hstack([np.ones([X_transformed_1.shape[0],1]), X_transformed_1])\n",
    "\n",
    "logit = sm.Logit(Y_1, X_transformed_1)\n",
    "fitted_model_demo = logit.fit_regularized()\n",
    "fitted_model_demo.summary()\n",
    "\n",
    "# alpha = 0.05\n",
    "# all features together are significant\n",
    "# Columns that are significant: 0,3,4,5,20,21,22,33,39,46,47,51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model to get the feature importances\n",
    "ind_q1 = independent_q1_alt.copy()\n",
    "ind_q1.columns = [np.arange(len(ind_q1.columns))]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ind_q1,dependent_class_alt,test_size = 0.2)\n",
    "\n",
    "estimators = [(\"normalize\", preprocessing.StandardScaler()),\n",
    "             (\"model\",LogisticRegression())]\n",
    "\n",
    "pipe = pipeline.Pipeline(estimators)\n",
    "\n",
    "param_grid = [{\"model\":[XGBClassifier()], \n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__max_depth\":[1,2,3,4,5],\"model__n_estimators\":[50,100,150,200],\"model__n_jobs\":[6]},\n",
    "              {\"model\": [RandomForestClassifier()],\n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__n_estimators\":[100,150,200],\"model__n_jobs\":[6]}]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=6)\n",
    "fitted_grid_1 = grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ranking of factors that are important for predicting comfort level, from most to least important\n",
    "indices = np.flip(fitted_grid_1.best_estimator_.named_steps[\"model\"].feature_importances_.argsort())\n",
    "\n",
    "# creating a list of factors that are statistically significant\n",
    "sig_list = [3,4,5,20,21,22,33,39,46,47,51]\n",
    "\n",
    "numbers = []\n",
    "for position,idx in enumerate(indices):\n",
    "    for num in sig_list:\n",
    "        if idx == num:\n",
    "            numbers.append(idx)\n",
    "\n",
    "independent_q1_alt.columns[numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can companies to do encourage their employees to seek treatment?\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up 2 dataframes for concatenating data\n",
    "omitted = pd.DataFrame(columns = [\"Question\",\"Answer\"])\n",
    "final = pd.DataFrame()\n",
    "\n",
    "# current_mh_coverage\n",
    "for i in [0,2,3]:\n",
    "    result = make_dummies(current_mh_coverage[i],\"Yes\")\n",
    "    final = pd.concat([final,result],axis = 1)\n",
    "\n",
    "result = make_dummies_iloc(current_mh_coverage[5],[2,-2,-1])\n",
    "final = pd.concat([final,result],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a table of independend variables for this question\n",
    "ind_variable = pd.concat([final,demographics],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# determining statistical significance using statsmodel Logistic Regression\n",
    "dep_question = \"Have you ever sought treatment for a mental health disorder from a mental health professional?\"\n",
    "\n",
    "X_1 = ind_variable\n",
    "Y_1 = df_2.loc[:,dep_question]\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_1)\n",
    "X_transformed_1 = scaler.transform(X_1)\n",
    "\n",
    "X_transformed_1 = np.hstack([np.ones([X_transformed_1.shape[0],1]), X_transformed_1])\n",
    "\n",
    "logit = sm.Logit(Y_1, X_transformed_1)\n",
    "fitted_model_sp = logit.fit_regularized()\n",
    "fitted_model_sp.summary()\n",
    "\n",
    "# alpha = 0.05\n",
    "# all features together are significant\n",
    "# Columns that are significant: 0,5,6,7,8,12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model to get the feature importance\n",
    "ind_alt = ind_variable.copy()\n",
    "ind_alt.columns = [np.arange(len(ind_alt.columns))]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ind_alt,df_2.loc[:,dep_question],test_size = 0.2)\n",
    "\n",
    "estimators = [(\"normalize\", preprocessing.StandardScaler()),\n",
    "             (\"model\",LogisticRegression())]\n",
    "\n",
    "pipe = pipeline.Pipeline(estimators)\n",
    "\n",
    "param_grid = [{\"model\":[XGBClassifier()], \n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__max_depth\":[1,2,3,4,5],\"model__n_estimators\":[50,100,150,200],\"model__n_jobs\":[6]},\n",
    "              {\"model\": [RandomForestClassifier()],\n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__n_estimators\":[100,150,200],\"model__n_jobs\":[6]}]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "fitted_grid_2 = grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking of factors that are important for predicting comfort level, from most to least important\n",
    "indices = np.flip(fitted_grid_2.best_estimator_.named_steps[\"model\"].feature_importances_.argsort())\n",
    "\n",
    "# displaying features that have statistical significance\n",
    "sig_list = [0,5,6,7,8,12]\n",
    "\n",
    "numbers = []\n",
    "for position,idx in enumerate(indices):\n",
    "    for num in sig_list:\n",
    "        if idx == num:\n",
    "            numbers.append(idx)\n",
    "\n",
    "ind_variable.columns[numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Please continue to Capstone modelling stage v.3-RNN for the remainder of the project.\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
