{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "# Basics\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import numpy as np\n",
    "\n",
    "# Graphs\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Preprocessing; model selection and evaluation\n",
    "from sklearn import pipeline, preprocessing\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# text handling\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# for custom countvectorizer with SpaCy lemmatization\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, VectorizerMixin\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# WordCloud\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "If we need to move virtual ENV to use Tensorflow we will need to install:\n",
    "\n",
    "1. spacy\n",
    "\n",
    "<code> conda install -c spacy spacy </code>\n",
    "\n",
    "2. 'en_core_web_md'\n",
    "\n",
    "<code> python -m spacy download en_core_web_md </code>\n",
    "\n",
    "3. wordcloud\n",
    "\n",
    "<code> conda install -c conda-forge wordcloud </code>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "data = pd.read_csv(\"saved_csv/df.csv\")\n",
    "data.drop(columns = \"Unnamed: 0\",inplace=True)\n",
    "\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Setting up data for analysis\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting questions into categories\n",
    "\n",
    "current_mh_coverage = [\"Does your employer provide mental health benefits as part of healthcare coverage?\",\n",
    "               \"Do you know the options for mental health care available under your employer-provided health coverage?\",\n",
    "               \"Has your employer ever formally discussed mental health (for example, as part of a wellness campaign or other official communication)?\",\n",
    "               \"Does your employer offer resources to learn more about mental health disorders and options for seeking help?\",\n",
    "               \"Is your anonymity protected if you choose to take advantage of mental health or substance abuse treatment resources provided by your employer?\",\n",
    "               \"If a mental health issue prompted you to request a medical leave from work, how easy or difficult would it be to ask for that leave?\"]\n",
    "\n",
    "previous_mh_coverage = [\"Have your previous employers provided mental health benefits?\",\n",
    "                        \"Were you aware of the options for mental health care provided by your previous employers?\",\n",
    "                        \"Did your previous employers ever formally discuss mental health (as part of a wellness campaign or other official communication)?\",\n",
    "                        \"Did your previous employers provide resources to learn more about mental health disorders and how to seek help?\",\n",
    "                        \"Was your anonymity protected if you chose to take advantage of mental health or substance abuse treatment resources with previous employers?\"]\n",
    "\n",
    "mh_status = [\"Do you currently have a mental health disorder?\",\n",
    "             \"Have you ever been diagnosed with a mental health disorder?\",'Anxiety Disorder', 'Mood Disorder', \n",
    "             'Psychotic Disorder','Eating Disorder', 'Neurodevelopmental Disorders','Personality Disorder', \n",
    "             'Obsessive-Compulsive Disorder','Post-Traumatic Stress Disorder', 'Dissociative Disorder',\n",
    "             'Substance-Related and Addictive Disorders', 'Other','Adjustment disorder',\n",
    "             \"Have you had a mental health disorder in the past?\",\n",
    "             \"Have you ever sought treatment for a mental health disorder from a mental health professional?\",\n",
    "             \"Do you have a family history of mental illness?\",\n",
    "             \"How willing would you be to share with friends and family that you have a mental illness?\",\n",
    "             \"Would you be willing to bring up a physical health issue with a potential employer in an interview?\"]\n",
    "\n",
    "witnessed_exp = [\"Have your observations of how another individual who discussed a mental health issue made you less likely to reveal a mental health issue yourself in your current workplace?\",\n",
    "                 \"Have you observed or experienced an unsupportive or badly handled response to a mental health issue in your current or previous workplace?\",\n",
    "                 \"Have you observed or experienced supportive or well handled response to a mental health issue in your current or previous workplace?\"]\n",
    "\n",
    "ratings = df.columns[df.columns.str.contains(\"Overall\")]\n",
    "\n",
    "demographics = [\"What is your age?\",\"What is your gender?\",\"What country do you live in?\",\n",
    "                \"What US state or territory do you live in?\",\"What is your race?\"]\n",
    "\n",
    "comfort_talking_current = [\"Would you feel comfortable discussing a mental health issue with your direct supervisor(s)?\",\n",
    "                           \"Have you ever discussed your mental health with your employer?\",\n",
    "                           \"Would you feel comfortable discussing a mental health issue with your coworkers?\",\n",
    "                           \"Have you ever discussed your mental health with coworkers?\",\n",
    "                           \"Have you ever had a coworker discuss their or another coworker's mental health with you?\",\n",
    "                           \"Would you feel more comfortable talking to your coworkers about your physical health or your mental health?\",\n",
    "                           \"Would you bring up your mental health with a potential employer in an interview?\",\n",
    "                           \"Are you openly identified at work as a person with a mental health issue?\"]\n",
    "\n",
    "comfort_talking_previous = [\"Would you have felt more comfortable talking to your previous employer about your physical health or your mental health?\",\n",
    "                            \"Would you have been willing to discuss your mental health with your direct supervisor(s)?\",\n",
    "                            \"Did you ever discuss your mental health with your previous employer?\",\n",
    "                            \"Would you have been willing to discuss your mental health with your coworkers at previous employers?\",\n",
    "                            \"Did you ever discuss your mental health with a previous coworker(s)?\",\n",
    "                            \"Did you ever have a previous coworker discuss their or another coworker's mental health with you?\",\n",
    "                            \"Would you bring up your mental health with a potential employer in an interview?\",\n",
    "                            \"Are you openly identified at work as a person with a mental health issue?\"]\n",
    "\n",
    "comfort_dependent_var = [\"Would you feel comfortable discussing a mental health issue with your direct supervisor(s)?\",\n",
    "                           \"Have you ever discussed your mental health with your employer?\",\n",
    "                           \"Would you feel comfortable discussing a mental health issue with your coworkers?\",\n",
    "                           \"Have you ever discussed your mental health with coworkers?\",\n",
    "                           \"Have you ever had a coworker discuss their or another coworker's mental health with you?\",\n",
    "                           \"Would you feel more comfortable talking to your coworkers about your physical health or your mental health?\",\n",
    "                           \"Would you have felt more comfortable talking to your previous employer about your physical health or your mental health?\",\n",
    "                           \"Would you have been willing to discuss your mental health with your direct supervisor(s)?\",\n",
    "                           \"Did you ever discuss your mental health with your previous employer?\",\n",
    "                           \"Would you have been willing to discuss your mental health with your coworkers at previous employers?\",\n",
    "                           \"Did you ever discuss your mental health with a previous coworker(s)?\",\n",
    "                           \"Did you ever have a previous coworker discuss their or another coworker's mental health with you?\",\n",
    "                           \"Would you bring up your mental health with a potential employer in an interview?\",\n",
    "                           \"Are you openly identified at work as a person with a mental health issue?\"]\n",
    "\n",
    "categories = [current_mh_coverage,previous_mh_coverage,mh_status,witnessed_exp,ratings,comfort_talking_current,comfort_talking_previous,comfort_dependent_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "------\n",
    "\n",
    "#### Independent Variables for Q1\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummies(question,columns_to_keep = 1):\n",
    "    \n",
    "    dummies = pd.get_dummies(df_2.loc[:,question])\n",
    "    for j in range(len(dummies.columns)):\n",
    "        name = question + \"__\" + dummies.columns[j]\n",
    "        dummies.rename(columns = {dummies.columns[j] : name},inplace=True)\n",
    "\n",
    "    result = dummies.iloc[:,[columns_to_keep]]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a copy\n",
    "df_2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preparing a table of independent variables for Q1\n",
    "independent_var = [current_mh_coverage,previous_mh_coverage,witnessed_exp,mh_status,ratings]\n",
    "dependent_var = [comfort_talking_current,comfort_talking_previous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up 2 dataframes for concatenating data\n",
    "omitted = pd.DataFrame(columns = [\"Question\",\"Answer\"])\n",
    "final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# current_mh_coverage\n",
    "for i in [0,2,3]:\n",
    "    result = make_dummies(current_mh_coverage[i],-1)\n",
    "    final = pd.concat([final,result],axis = 1)\n",
    "\n",
    "for i in [2,-2,-1]:\n",
    "    result = make_dummies(current_mh_coverage[5],i)\n",
    "    final = pd.concat([final,result],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# witnessed_exp\n",
    "result = make_dummies(witnessed_exp[0],-1)\n",
    "final = pd.concat([final,result],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mh_status\n",
    "old_answer_1 = \"Possibly\"\n",
    "old_answer_2 = \"-1\"\n",
    "answer = \"Don't Know\"\n",
    "\n",
    "to_dummy = []\n",
    "for i in [0,1,-5,-3,-1]:\n",
    "    to_dummy.append(mh_status[i])\n",
    "    \n",
    "df_2.loc[:,to_dummy[0]][df_2.loc[:,to_dummy[0]]==old_answer_1]=answer\n",
    "\n",
    "df_2.loc[:,to_dummy[2]][df_2.loc[:,to_dummy[2]]==old_answer_1]=answer\n",
    "df_2.loc[:,to_dummy[2]][df_2.loc[:,to_dummy[2]]==old_answer_2]=answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mh_status\n",
    "result = make_dummies(to_dummy[0],-1)\n",
    "final = pd.concat([final,result],axis = 1)\n",
    "\n",
    "for i in [2,3,4,5,6,7,8,9,10,11,12,13,15,17]:\n",
    "    final = pd.concat([final,df_2.loc[:,mh_status[i]]],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings\n",
    "for i in [1,4]:\n",
    "    final = pd.concat([final,df_2.loc[:,ratings[i]]],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_q1 = final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependent Variables for Q1\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a table of dependent variables\n",
    "dependent = df.loc[:,comfort_dependent_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = [0,2,5,6,7,9,12]\n",
    "columns_to_join = [1,3,4,8,10,11,13]\n",
    "\n",
    "final_dep = pd.get_dummies(dependent.iloc[:,num_list])\n",
    "\n",
    "for i in columns_to_join:\n",
    "    result = pd.get_dummies(dependent.iloc[:,i])\n",
    "    final_dep = pd.concat([final_dep,result],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting classes\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models Used:\n",
    "\n",
    "- KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KMeans Clustering\n",
    "X_alt = final_dep\n",
    "\n",
    "inertia = []\n",
    "\n",
    "for num in range(1,21):\n",
    "    k_means_model = KMeans(n_clusters = num)\n",
    "    k_means_model.fit(X_alt)\n",
    "\n",
    "    y_pred2_alt = k_means_model.predict(X_alt)\n",
    "    inertia.append(k_means_model.inertia_)\n",
    "\n",
    "# b_alt = pd.DataFrame(y_pred2_alt)\n",
    "# b_alt[0].groupby(b_alt[0]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(1,21),inertia);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_model = KMeans(n_clusters = 4)\n",
    "k_means_model.fit(X_alt)\n",
    "\n",
    "y_pred2_alt = k_means_model.predict(X_alt)\n",
    "\n",
    "b_alt = pd.DataFrame(y_pred2_alt)\n",
    "b_alt[0].groupby(b_alt[0]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_alt.rename(columns = {0 : \"Classes\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b_alt.to_csv(\"saved_csv/q1_dependent_alt.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is a factor of randomness in KMeans Clustering, the code will generate different counts with each iteration. Since the goal is to generate a cluster of 4 classes with an approximately normal distribution, the following cluster count will be used for the analysis:\n",
    "\n",
    "|Class|Count|\n",
    "|------|------|\n",
    "|0|143|\n",
    "|1|490|\n",
    "|2|386|\n",
    "|3|154|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Qualitative insights for the tech industry to improve MH support for employees\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing the dependent variables\n",
    "dependent_class = pd.read_csv(\"saved_csv/q1_dependent_alt.csv\")\n",
    "dependent_class.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplifing dependent variables to 0/1\n",
    "dependent_class_alt = dependent_class.copy()\n",
    "\n",
    "dependent_class_alt[dependent_class_alt < 2] = 0\n",
    "dependent_class_alt[dependent_class_alt >= 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: https://github.com/mpavlovic/spacy-vectorizers\n",
    "# create a custom countvectorizer with SpaCy lemmatization\n",
    "\n",
    "class SpacyPipeInitializer(object):\n",
    "    def __init__(self, nlp, join_str=\" \", batch_size=10000, n_threads=2):\n",
    "        self.nlp = nlp\n",
    "        self.join_str = join_str\n",
    "        self.batch_size = batch_size\n",
    "        self.n_threads = n_threads\n",
    "        \n",
    "class SpacyPipeProcessor(SpacyPipeInitializer):\n",
    "    def __init__(self, nlp, multi_iters=False, join_str=\" \", batch_size=10000, n_threads=2):\n",
    "        super(SpacyPipeProcessor, self).__init__(nlp, join_str, batch_size, n_threads)\n",
    "        self.multi_iters = multi_iters\n",
    "    \n",
    "    def __call__(self, raw_documents):\n",
    "        docs_generator = self.nlp.pipe(raw_documents, batch_size=self.batch_size, n_threads=self.n_threads)\n",
    "        return docs_generator if self.multi_iters == False else list(docs_generator)\n",
    "    \n",
    "class SpacyLemmaCountVectorizer(CountVectorizer):\n",
    "    \n",
    "    def __init__(self, input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None,\n",
    "                 lowercase=True, preprocessor=None, tokenizer=None,\n",
    "                 stop_words=None, token_pattern=r\"(?u)[^\\r\\n ]+\",\n",
    "                 ngram_range=(1, 1), analyzer='word',\n",
    "                 max_df=1.0, min_df=1, max_features=None,\n",
    "                 vocabulary=None, binary=False, dtype=np.int64, \n",
    "                 nlp=None, ignore_chars='!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~', \n",
    "                 join_str=\" \", use_pron=False):\n",
    "        \n",
    "        super().__init__(input, encoding, decode_error, strip_accents, \n",
    "                                                   lowercase, preprocessor, tokenizer,\n",
    "                                                   stop_words, token_pattern, ngram_range, \n",
    "                                                   analyzer, max_df, min_df, max_features,\n",
    "                                                   vocabulary, binary, dtype)\n",
    "        self.ignore_chars = ignore_chars\n",
    "        self.join_str = ' ' # lemmas have to be joined for splitting\n",
    "        self.use_pron = use_pron\n",
    "        self.translate_table = dict((ord(char), None) for char in self.ignore_chars)\n",
    "        \n",
    "    def lemmatize_from_docs(self, docs):\n",
    "        for doc in docs:\n",
    "            lemmas_gen = (token.lemma_.translate(self.translate_table) if self.use_pron or token.lemma_!='-PRON-' else token.lower_.translate(self.translate_table) for token in doc)  # generator expression\n",
    "            yield self.join_str.join(lemmas_gen) if self.join_str is not None else [lemma for lemma in lemmas_gen]\n",
    "    \n",
    "    def build_tokenizer(self):\n",
    "        return lambda doc: doc.split()\n",
    "    \n",
    "    def transform(self, spacy_docs):\n",
    "        raw_documents = self.lemmatize_from_docs(spacy_docs)\n",
    "        return super(SpacyLemmaCountVectorizer, self).transform(raw_documents)\n",
    "    \n",
    "    def fit_transform(self, spacy_docs, y=None):\n",
    "        raw_documents = self.lemmatize_from_docs(spacy_docs)\n",
    "        return super(SpacyLemmaCountVectorizer, self).fit_transform(raw_documents, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grabbing the responses as independent variables\n",
    "corpus = df.iloc[:,-9]\n",
    "\n",
    "# dependent variables -> ratings\n",
    "industry_rating = df.iloc[:,-10]\n",
    "\n",
    "# make the ratings from 1-5 to 0-4\n",
    "industry_rating = industry_rating - 1\n",
    "\n",
    "# combining all info into one table\n",
    "table_q4_alt = pd.concat([corpus,industry_rating,dependent_class_alt],axis=1)\n",
    "\n",
    "# dropping columns that did not answer the question\n",
    "index = table_q4_alt[table_q4_alt.iloc[:,0]==\"Did not answer\"].index\n",
    "\n",
    "table_q4_alt.drop(index,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customization stopwords to filter out some words\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"mental\",\"health\",\"issue\",\"work\",\n",
    "                  \"take\",\"hour\",\"tech\",\"industry\",\"people\",\"employee\"])\n",
    "\n",
    "# CountVectorizer with SpaCy Lemmatization\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "spp = SpacyPipeProcessor(nlp, n_threads=1, multi_iters=True)\n",
    "spacy_docs = spp(table_q4_alt.iloc[:,0]);\n",
    "\n",
    "slcv = SpacyLemmaCountVectorizer(min_df=3,stop_words=stopwords, ngram_range=(1, 3), ignore_chars='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "slcv.fit(spacy_docs)\n",
    "count_vectors = slcv.transform(spacy_docs); count_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pulling out the list of parsed words and put them into a wordcloud\n",
    "list_of_words = slcv.vocabulary_.keys()\n",
    "list_of_words = list(list_of_words)\n",
    "list_of_words.sort()\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(\" \".join(list_of_words))\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 - Factors that affect comfort level in discussing MH at workplace\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummies_q(question,num,drop_pattern = 1):\n",
    "    '''\n",
    "    This function creates dummy variable for text responses.\n",
    "    It also creates a table of answers that are omitted in the table in preparation for analysis,\n",
    "        since n-1 answer for each question is needed to avoid multicollinearity.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    question_omitted: a table with question and the answer that has been omitted to avoid multicollinearity.\n",
    "    result: a table of dummy variables.\n",
    "    '''\n",
    "    d = {}\n",
    "    question_omitted = pd.DataFrame(columns = [\"Question\",\"Answer\"])\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    d[num] = pd.get_dummies(df_2.loc[:,question])\n",
    "    if drop_pattern == 1:\n",
    "        question_omitted = question_omitted.append({\"Question\":question, \"Answer\": d[num].columns[-2]},ignore_index = True)\n",
    "        d[num].drop(columns = d[num].columns[0],inplace = True)\n",
    "    elif drop_pattern == 2:\n",
    "        question_omitted = question_omitted.append({\"Question\":question, \"Answer\": d[num].columns[3]},ignore_index = True)\n",
    "        d[num].drop(columns = d[num].columns[3],inplace = True)\n",
    "\n",
    "    for i in range(len(d[num].columns)):\n",
    "        name = question + \"__\" + d[num].columns[i]\n",
    "        d[num].rename(columns = {d[num].columns[i] : name},inplace=True)\n",
    "\n",
    "    result = pd.concat([result,d[num]],axis=1)\n",
    "        \n",
    "    return question_omitted,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing the dependent variables\n",
    "dependent_class = pd.read_csv(\"saved_csv/q1_dependent_alt.csv\")\n",
    "dependent_class.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What is your gender?\n",
    "question_omitted,gender = make_dummies_q(df_2.columns[-7],-7,drop_pattern = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping countries into 5 continents\n",
    "country_names = df_2.iloc[:,-6].groupby(df_2.iloc[:,-6]).count().index\n",
    "\n",
    "df_2[\"countries_continent\"] = df_2.iloc[:,-6]\n",
    "\n",
    "north_am = [8,34,55]\n",
    "south_am = [0,6,9,56]\n",
    "asia = [3,20,23,24,26,28,29,39,43,44,46]\n",
    "africa = [14,30,33,37,48]\n",
    "europe = [2,4,5,7,10,11,13,15,16,17,18,19,21,22,25,27,31,32,35,38,40,41,42,45,47,49,50,51,52,53,54]\n",
    "oceania = [1,36]\n",
    "did_not_answer = [12]\n",
    "\n",
    "continents = [north_am,south_am,asia,africa,europe,oceania,did_not_answer]\n",
    "names = [\"North America\", \"South America\", \"Asia\", \"Africa\", \"Europe\", \"Oceania\", \"Did not answer\"]\n",
    "\n",
    "for position,continent in enumerate(continents):\n",
    "    for num in np.flip(continent):\n",
    "        df_2.loc[:,\"countries_continent\"][df_2.loc[:,\"countries_continent\"]== country_names[num]]=names[position]\n",
    "        \n",
    "countries = df_2.iloc[:,-1]\n",
    "\n",
    "countries = pd.get_dummies(countries)\n",
    "countries.drop(columns = \"North America\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is your race?\n",
    "races = pd.get_dummies(df_2.iloc[:,-5])\n",
    "races.drop(columns = \"Caucasian\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_size\n",
    "company_size = pd.get_dummies(df_2.iloc[:,2])\n",
    "company_size.drop(columns=\"0\",inplace=True)\n",
    "size = company_size.iloc[:,[3,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a table of demographics\n",
    "demographics = pd.concat([gender,countries,races,df_2.iloc[:,[-9,1,3,4]],size],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating features/independent variables\n",
    "independent_q1_alt = pd.concat([independent_q1,demographics],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# p-value WITH DEMOGRAPHICS\n",
    "\n",
    "X_1 = independent_q1_alt\n",
    "Y_1 = dependent_class_alt\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_1)\n",
    "X_transformed_1 = scaler.transform(X_1)\n",
    "\n",
    "X_transformed_1 = np.hstack([np.ones([X_transformed_1.shape[0],1]), X_transformed_1])\n",
    "\n",
    "logit = sm.Logit(Y_1, X_transformed_1)\n",
    "fitted_model_demo = logit.fit_regularized()\n",
    "fitted_model_demo.summary()\n",
    "\n",
    "# alpha = 0.05\n",
    "# all features together are significant\n",
    "# Columns that are significant: 3,4,5,20,21,22,33,39,46,47,51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of questions that are statistically significant\n",
    "num_list = [3,4,5,20,21,22,33,39,46,47,51]\n",
    "q1_ind_sig = independent_q1_alt.iloc[:,num_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model using the questions that are statistically significant\n",
    "X_train, X_test, y_train, y_test = train_test_split(q1_ind_sig,dependent_class_alt,test_size = 0.2)\n",
    "\n",
    "estimators = [(\"normalize\", preprocessing.StandardScaler()),\n",
    "             (\"model\",LogisticRegression())]\n",
    "\n",
    "pipe = pipeline.Pipeline(estimators)\n",
    "\n",
    "param_grid = [{\"model\":[XGBClassifier()], \n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__max_depth\":[1,2,3,4,5],\"model__n_estimators\":[50,100,150,200],\"model__n_jobs\":[6]},\n",
    "              {\"model\": [RandomForestClassifier()],\n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__n_estimators\":[100,150,200],\"model__n_jobs\":[6]}]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=6)\n",
    "fitted_grid_1 = grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_1.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ranking of factors that are important for predicting comfort level, from most to least important\n",
    "indices = np.flip(fitted_grid_1.best_estimator_.named_steps[\"model\"].feature_importances_.argsort())\n",
    "\n",
    "q1_ind_sig.columns[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can companies to do encourage their employees to seek treatment?\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up 2 dataframes for concatenating data\n",
    "omitted = pd.DataFrame(columns = [\"Question\",\"Answer\"])\n",
    "final = pd.DataFrame()\n",
    "\n",
    "# current_mh_coverage\n",
    "for i in [0,2,3]:\n",
    "    result = make_dummies(current_mh_coverage[i],-1)\n",
    "    final = pd.concat([final,result],axis = 1)\n",
    "\n",
    "for i in [2,-2,-1]:\n",
    "    result = make_dummies(current_mh_coverage[5],i)\n",
    "    final = pd.concat([final,result],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_variable = pd.concat([final,demographics],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dep_question = \"Have you ever sought treatment for a mental health disorder from a mental health professional?\"\n",
    "\n",
    "X_1 = ind_variable\n",
    "Y_1 = df_2.loc[:,dep_question]\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_1)\n",
    "X_transformed_1 = scaler.transform(X_1)\n",
    "\n",
    "X_transformed_1 = np.hstack([np.ones([X_transformed_1.shape[0],1]), X_transformed_1])\n",
    "\n",
    "logit = sm.Logit(Y_1, X_transformed_1)\n",
    "fitted_model_sp = logit.fit_regularized()\n",
    "fitted_model_sp.summary()\n",
    "\n",
    "# alpha = 0.05\n",
    "# all features together are significant\n",
    "# Columns that are significant: 0,5,6,7,8,12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of questions that are statistically significant\n",
    "num_list = [0,5,6,7,8,12]\n",
    "ind_sig = ind_variable.iloc[:,num_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model using the questions that are statistically significant\n",
    "X_train, X_test, y_train, y_test = train_test_split(ind_sig,df_2.loc[:,dep_question],test_size = 0.2)\n",
    "\n",
    "estimators = [(\"normalize\", preprocessing.StandardScaler()),\n",
    "             (\"model\",LogisticRegression())]\n",
    "\n",
    "pipe = pipeline.Pipeline(estimators)\n",
    "\n",
    "param_grid = [{\"model\":[XGBClassifier()], \n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__max_depth\":[1,2,3,4,5],\"model__n_estimators\":[50,100,150,200],\"model__n_jobs\":[6]},\n",
    "              {\"model\": [RandomForestClassifier()],\n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__n_estimators\":[100,150,200],\"model__n_jobs\":[6]},\n",
    "              {\"model\": [LogisticRegression()],\n",
    "               \"normalize\": [preprocessing.StandardScaler(), preprocessing.MinMaxScaler(), None],\n",
    "               \"model__penalty\":[\"l1\",\"l2\"],\"model__C\":[1e-04,1e-02,0.1,1,10,100]}]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "fitted_grid_2 = grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_grid_2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking of factors that are important for predicting comfort level, from most to least important\n",
    "indices = np.flip(fitted_grid_2.best_estimator_.named_steps[\"model\"].feature_importances_.argsort())\n",
    "\n",
    "ind_sig.columns[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative insights for the tech industry to improve MH support for employees, based on comfort level\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customization stopwords to filter out some words\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"mental\",\"health\",\"issue\",\"work\",\n",
    "                  \"take\",\"hour\",\"tech\",\"industry\",\"people\",\"employee\",\"open\",\"openly\",\"make\",\"long\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num in range(2):\n",
    "\n",
    "    table = table_q4_alt[table_q4_alt.iloc[:,-1]==num]\n",
    "\n",
    "    # CountVectorizer with SpaCy Lemmatization\n",
    "    spp = SpacyPipeProcessor(nlp, n_threads=1, multi_iters=True)\n",
    "    spacy_docs = spp(table.iloc[:,0]);\n",
    "\n",
    "    slcv = SpacyLemmaCountVectorizer(min_df=3,stop_words=stopwords, ngram_range=(1, 3), ignore_chars='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "    slcv.fit(spacy_docs)\n",
    "    count_vectors = slcv.transform(spacy_docs); count_vectors\n",
    "\n",
    "    # Pulling out the list of parsed words and put them into a wordcloud\n",
    "    list_of_words = slcv.vocabulary_.keys()\n",
    "    list_of_words = list(list_of_words)\n",
    "    list_of_words.sort()\n",
    "\n",
    "    wordcloud = WordCloud(background_color=\"white\").generate(\" \".join(list_of_words))\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use RNN to do something with the words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dependent_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "lemmatized = []\n",
    "\n",
    "for idx in range(len(corpus)):\n",
    "    doc = nlp(corpus[idx])\n",
    "    lemmatized.append(\" \".join([token.lemma_ for token in doc]))\n",
    "\n",
    "ind_var = pd.DataFrame(lemmatized,columns = [\"Responses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(lemmatized).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, GRU, CuDNNLSTM, BatchNormalization, Flatten, Embedding\n",
    "                                                               # this is faster than LSTM\n",
    "                                                               # but you cannot change the activation function\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.concat([corpus,dependent_class_alt],axis=1)\n",
    "\n",
    "# dropping columns that did not answer the question\n",
    "index = table[table.iloc[:,0]==\"Did not answer\"].index\n",
    "\n",
    "table.drop(index,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<789x755 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7266 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# customization stopwords to filter out some words\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"mental\",\"health\",\"issue\",\"work\",\n",
    "                  \"take\",\"hour\",\"tech\",\"industry\",\"people\",\"employee\"])\n",
    "\n",
    "# CountVectorizer with SpaCy Lemmatization\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "spp = SpacyPipeProcessor(nlp, n_threads=1, multi_iters=True)\n",
    "spacy_docs = spp(table.iloc[:,0]);\n",
    "\n",
    "slcv = SpacyLemmaCountVectorizer(min_df=3,stop_words=stopwords, ngram_range=(1, 3), ignore_chars='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "slcv.fit(spacy_docs)\n",
    "count_vectors = slcv.transform(spacy_docs); count_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(count_vectors,table.iloc[:,1].values,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_101 (LSTM)              (None, 755, 64)           16896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 755, 64)           256       \n",
      "_________________________________________________________________\n",
      "lstm_102 (LSTM)              (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 52,578\n",
      "Trainable params: 52,322\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Train on 631 samples, validate on 158 samples\n",
      "Epoch 1/20\n",
      "631/631 [==============================] - 36s 57ms/step - loss: 0.7083 - acc: 0.5372 - val_loss: 0.6923 - val_acc: 0.5316\n",
      "Epoch 2/20\n",
      "631/631 [==============================] - 36s 57ms/step - loss: 0.6999 - acc: 0.5372 - val_loss: 0.6917 - val_acc: 0.5316\n",
      "Epoch 3/20\n",
      "631/631 [==============================] - 35s 55ms/step - loss: 0.7027 - acc: 0.5578 - val_loss: 0.6914 - val_acc: 0.5316\n",
      "Epoch 4/20\n",
      "631/631 [==============================] - 33s 52ms/step - loss: 0.6891 - acc: 0.5769 - val_loss: 0.6911 - val_acc: 0.5316\n",
      "Epoch 5/20\n",
      "631/631 [==============================] - 32s 51ms/step - loss: 0.7034 - acc: 0.5436 - val_loss: 0.6911 - val_acc: 0.5316\n",
      "Epoch 6/20\n",
      "631/631 [==============================] - 31s 50ms/step - loss: 0.6853 - acc: 0.5658 - val_loss: 0.6911 - val_acc: 0.5316\n",
      "Epoch 7/20\n",
      "631/631 [==============================] - 30s 48ms/step - loss: 0.6934 - acc: 0.5531 - val_loss: 0.6911 - val_acc: 0.5316\n",
      "Epoch 8/20\n",
      "631/631 [==============================] - 30s 47ms/step - loss: 0.6876 - acc: 0.5721 - val_loss: 0.6910 - val_acc: 0.5316\n",
      "Epoch 9/20\n",
      "631/631 [==============================] - 31s 49ms/step - loss: 0.6919 - acc: 0.5658 - val_loss: 0.6911 - val_acc: 0.5316\n",
      "Epoch 10/20\n",
      "631/631 [==============================] - 31s 49ms/step - loss: 0.7006 - acc: 0.5610 - val_loss: 0.6911 - val_acc: 0.5316\n",
      "Epoch 11/20\n",
      "631/631 [==============================] - 29s 47ms/step - loss: 0.6841 - acc: 0.5626 - val_loss: 0.6914 - val_acc: 0.5316\n",
      "Epoch 12/20\n",
      "631/631 [==============================] - 32s 50ms/step - loss: 0.6878 - acc: 0.5547 - val_loss: 0.6916 - val_acc: 0.5316\n",
      "Epoch 13/20\n",
      "631/631 [==============================] - 30s 47ms/step - loss: 0.6908 - acc: 0.5658 - val_loss: 0.6912 - val_acc: 0.5316\n",
      "Epoch 14/20\n",
      "631/631 [==============================] - 29s 47ms/step - loss: 0.6791 - acc: 0.5689 - val_loss: 0.6912 - val_acc: 0.5316\n",
      "Epoch 15/20\n",
      "631/631 [==============================] - 29s 46ms/step - loss: 0.6902 - acc: 0.5689 - val_loss: 0.6913 - val_acc: 0.5316\n",
      "Epoch 16/20\n",
      "631/631 [==============================] - 29s 47ms/step - loss: 0.6911 - acc: 0.5674 - val_loss: 0.6916 - val_acc: 0.5316\n",
      "Epoch 17/20\n",
      "631/631 [==============================] - 30s 47ms/step - loss: 0.6857 - acc: 0.5674 - val_loss: 0.6907 - val_acc: 0.5316\n",
      "Epoch 18/20\n",
      "631/631 [==============================] - 29s 47ms/step - loss: 0.6921 - acc: 0.5642 - val_loss: 0.6910 - val_acc: 0.5316\n",
      "Epoch 19/20\n",
      "631/631 [==============================] - 29s 47ms/step - loss: 0.6878 - acc: 0.5563 - val_loss: 0.6913 - val_acc: 0.5316\n",
      "Epoch 20/20\n",
      "631/631 [==============================] - 29s 46ms/step - loss: 0.6867 - acc: 0.5753 - val_loss: 0.6908 - val_acc: 0.5316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c66e2a358>"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(count_vectors,table.iloc[:,1].values,test_size = 0.2)\n",
    "\n",
    "X_train = X_train.toarray().reshape(631,755,1)\n",
    "y_train = y_train.reshape(631,1)\n",
    "X_test = X_test.toarray().reshape(158,755,1)\n",
    "y_test = y_test.reshape(158,1)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(64,activation=\"relu\", input_shape = (X_train.shape[1:]), return_sequences=True, dropout=0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(64, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "# setting up SGD (optimizer) hyperparameters\n",
    "sgd = SGD(lr=0.01,decay=0.0, momentum = 0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer = sgd,\n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train,y_train, batch_size = 48, epochs = 20, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
