{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project\n",
    "-------\n",
    "\n",
    "### Stage 2 - Modelling phase - Neural Networks\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages and data\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible packages that need to be installed:\n",
    "\n",
    "1. Hyperas\n",
    "\n",
    "<code> conda install -c jaikumarm hyperas </code>\n",
    "\n",
    "2. mlxtend\n",
    "\n",
    "<code> conda install -c conda-forge mlxtend </code>\n",
    "\n",
    "These packages are from the previous notebook. If virtual environments are used for neural network, however, the following packages will need to be installed in order for the notebook to run properly.\n",
    "\n",
    "3. SpaCy\n",
    "\n",
    "<code> conda install -c spacy spacy </code>\n",
    "\n",
    "4. 'en_core_web_md' - library used in SpaCy\n",
    "\n",
    "<code> python -m spacy download en_core_web_md </code>\n",
    "\n",
    "5. wordcloud\n",
    "\n",
    "<code> conda install -c conda-forge wordcloud </code>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "\n",
    "# Hyperas/TensorFlow\n",
    "# the __future__ import command must be in the beginning of the notebook\n",
    "from __future__ import print_function\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from tensorflow.python.keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Basics\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import numpy as np\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Preprocessing; model selection and evaluation\n",
    "from sklearn import pipeline, preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Modelling\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# text handling\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# for custom countvectorizer with SpaCy lemmatization\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, VectorizerMixin\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# WordCloud\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "If we need to move virtual ENV to use Tensorflow we will need to install:\n",
    "\n",
    "1. spacy\n",
    "\n",
    "<code> conda install -c spacy spacy </code>\n",
    "\n",
    "2. 'en_core_web_md'\n",
    "\n",
    "<code> python -m spacy download en_core_web_md </code>\n",
    "\n",
    "3. wordcloud\n",
    "\n",
    "<code> conda install -c conda-forge wordcloud </code>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "data = pd.read_csv(\"saved_csv/df.csv\")\n",
    "data.drop(columns = \"Unnamed: 0\",inplace=True)\n",
    "\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model to predict comfort level using text responses\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the responses as independent variables\n",
    "corpus = df.iloc[:,-9]\n",
    "\n",
    "# grabbing the dependent variables\n",
    "# dependent_class = pd.read_csv(\"saved_csv/q1_dependent_alt.csv\")\n",
    "# dependent_class.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "# dependent_class_alt = dependent_class.copy()\n",
    "\n",
    "# dependent_class_alt[dependent_class_alt < 2] = 0\n",
    "# dependent_class_alt[dependent_class_alt >= 2] = 1\n",
    "\n",
    "question = \"Would you feel comfortable discussing a mental health issue with your coworkers?\"\n",
    "\n",
    "answers = [\"Maybe\",\"No\",\"Not Applicable\",\"Yes\"]\n",
    "\n",
    "dep = df[question].copy()\n",
    "\n",
    "for num in range(len(answers)):\n",
    "    if num != 3:\n",
    "        dep[dep==answers[num]] = 0 #No\n",
    "    else:\n",
    "        dep[dep==answers[num]] = 1 #Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a table with both independent and dependent variables\n",
    "table = pd.concat([corpus,dep],axis=1)\n",
    "\n",
    "# dropping columns that did not answer the question\n",
    "index = table[table.iloc[:,0]==\"Did not answer\"].index\n",
    "\n",
    "table.drop(index,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "words = [\"aren't\",\"can't\",\"can\",\"cannot\",\"could\",\"couldn't\",\"did\",\"didn't\",\"doing\",\"don't\",\"hasn't\",\"hadn't\",\"shan't\"]\n",
    "for word in words:\n",
    "    stopwords.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<789x1130 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 11996 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text processing to prepare data for RNN\n",
    "\n",
    "# Lemmatization using SpaCy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for num in range(len(table)):\n",
    "    doc = nlp(table.iloc[num,0])\n",
    "\n",
    "    sentence = []\n",
    "    for token in doc:\n",
    "        sentence.append(token.lemma_)\n",
    "\n",
    "    sentences.append(\" \".join(sentence))\n",
    "\n",
    "# Processing text with TfidfVectorizer\n",
    "tf_model = TfidfVectorizer(stop_words=stopwords,ngram_range=(1,3), min_df=3)\n",
    "tf_vectors = tf_model.fit_transform(sentences); tf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving files to be loaded in Hyperas functions\n",
    "np.save(\"saved_csv/tf_vectors.npy\", tf_vectors.toarray(), allow_pickle=True, fix_imports=True)\n",
    "\n",
    "table.to_csv(\"saved_csv/table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.layers.core import Dense, Dropout, Activation, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import LSTM, BatchNormalization\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import Conv1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import MaxPooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import GlobalAveragePooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import warnings\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import pipeline, preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from mlxtend.classifier import StackingCVClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.neighbors import KNeighborsClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.wrappers.scikit_learn import KerasClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from xgboost import XGBClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import spacy\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.feature_extraction.text import CountVectorizer, VectorizerMixin\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.base import TransformerMixin, BaseEstimator\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from scipy.sparse import csr_matrix\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from os import path\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from PIL import Image\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'LSTM': hp.choice('LSTM', [32,64,96,128]),\n",
      "        'activation': hp.choice('activation', [\"relu\",\"elu\"]),\n",
      "        'dropout': hp.uniform('dropout', 0,1),\n",
      "        'LSTM_1': hp.choice('LSTM_1', [32,64,96,128]),\n",
      "        'activation_1': hp.choice('activation_1', [\"relu\",\"elu\"]),\n",
      "        'dropout_1': hp.uniform('dropout_1', 0,1),\n",
      "        'LSTM_2': hp.choice('LSTM_2', [32,64,96,128]),\n",
      "        'activation_2': hp.choice('activation_2', [\"relu\",\"elu\"]),\n",
      "        'dropout_2': hp.uniform('dropout_2', 0,1),\n",
      "        'activation_3': hp.choice('activation_3', [\"softmax\",\"sigmoid\"]),\n",
      "        'lr': hp.uniform('lr', 0,0.01),\n",
      "        'batch_size': hp.choice('batch_size', [16, 32, 64]),\n",
      "        'epochs': hp.choice('epochs', [5, 10, 15]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: \"\"\"\n",
      "  3: Data providing function:\n",
      "  4: \n",
      "  5: This function is separated from create_model() so that hyperopt\n",
      "  6: won't reload data for each evaluation run.\n",
      "  7: \"\"\"\n",
      "  8: tf_vectors = np.load(\"saved_csv/tf_vectors.npy\")\n",
      "  9: \n",
      " 10: table = pd.read_csv(\"saved_csv/table.csv\")\n",
      " 11: table.drop(columns = \"Unnamed: 0\",inplace=True)\n",
      " 12: \n",
      " 13: x_train, x_test, y_train, y_test = train_test_split(tf_vectors,table.iloc[:,1].values,test_size = 0.2)\n",
      " 14: \n",
      " 15: x_train = x_train.reshape(631,1130,1)\n",
      " 16: y_train = y_train.reshape(631,1)\n",
      " 17: x_test = x_test.reshape(158,1130,1)\n",
      " 18: y_test = y_test.reshape(158,1)\n",
      " 19: \n",
      " 20: \n",
      " 21: \n",
      " 22: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     \"\"\"\n",
      "  13:     model = Sequential()\n",
      "  14: \n",
      "  15:     model.add(LSTM(space['LSTM'],activation=space['activation'], \n",
      "  16:                    input_shape = (x_train.shape[1:]), return_sequences=True, dropout=space['dropout']))\n",
      "  17:     model.add(BatchNormalization())\n",
      "  18: \n",
      "  19:     model.add(LSTM(space['LSTM_1'], activation=space['activation_1'], dropout=space['dropout_1']))\n",
      "  20:     model.add(BatchNormalization())\n",
      "  21: \n",
      "  22:     model.add(Dense(space['LSTM_2'], activation=space['activation_2']))\n",
      "  23:     model.add(Dropout(space['dropout_2']))\n",
      "  24: \n",
      "  25:     model.add(Dense(2, activation=space['activation_3']))\n",
      "  26: \n",
      "  27:     # setting up optimizer hyperparameters\n",
      "  28:     sgd = SGD(lr=space['lr'],decay=0.0, momentum = 0.0, nesterov=False, clipnorm=2.0)\n",
      "  29: \n",
      "  30:     # compile model\n",
      "  31:     model.compile(loss=\"sparse_categorical_crossentropy\", optimizer = sgd, metrics = [\"accuracy\"])\n",
      "  32: \n",
      "  33:     es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=2, verbose=1)\n",
      "  34: \n",
      "  35:     result = model.fit(x_train,y_train, batch_size = space['batch_size'], epochs = space['epochs'], \n",
      "  36:                        callbacks = [es], validation_split=0.2)\n",
      "  37: \n",
      "  38:     validation_acc = np.amax(result.history['val_acc']) \n",
      "  39:     print('Best validation acc of epoch:', validation_acc)\n",
      "  40:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      "  41: \n",
      "Train on 504 samples, validate on 127 samples      \n",
      "Epoch 1/5                                          \n",
      " 64/504 [==>...........................]           \n",
      " - ETA: 40s - loss: 0.7025 - acc: 0.5312           \n",
      "                                                  \n",
      "128/504 [======>.......................]           \n",
      " - ETA: 28s - loss: 0.7677 - acc: 0.5391           \n",
      "                                                  \n",
      "192/504 [==========>...................]           \n",
      " - ETA: 22s - loss: 0.8503 - acc: 0.5417           \n",
      "                                                  \n",
      "256/504 [==============>...............]           \n",
      " - ETA: 17s - loss: 0.8459 - acc: 0.5391           \n",
      "                                                  \n",
      "320/504 [==================>...........]           \n",
      " - ETA: 12s - loss: 0.8293 - acc: 0.5375           \n",
      "                                                  \n",
      "384/504 [=====================>........]           \n",
      " - ETA: 8s - loss: 0.8295 - acc: 0.5208            \n",
      "                                                   \n",
      "448/504 [=========================>....]           \n",
      " - ETA: 3s - loss: 0.8389 - acc: 0.5290            \n",
      "                                                   \n",
      "504/504 [==============================]           \n",
      " - 35s 69ms/step - loss: 0.8615 - acc: 0.5238 - val_loss: 0.6767 - val_acc: 0.6614\n",
      "\n",
      "Epoch 2/5                                          \n",
      " 64/504 [==>...........................]           \n",
      " - ETA: 25s - loss: 0.7269 - acc: 0.5156           \n",
      "                                                  \n",
      "128/504 [======>.......................]           \n",
      " - ETA: 21s - loss: 0.8185 - acc: 0.5547           \n",
      "                                                  \n",
      "192/504 [==========>...................]           \n",
      " - ETA: 17s - loss: 0.8088 - acc: 0.5156           \n",
      "                                                  \n",
      "256/504 [==============>...............]           \n",
      " - ETA: 14s - loss: 0.7891 - acc: 0.5234           \n",
      "                                                  \n",
      "320/504 [==================>...........]           \n",
      " - ETA: 10s - loss: 0.7708 - acc: 0.5469           \n",
      "                                                  \n",
      "384/504 [=====================>........]           \n",
      " - ETA: 6s - loss: 0.7899 - acc: 0.5521            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "448/504 [=========================>....]           \n",
      " - ETA: 3s - loss: 0.7678 - acc: 0.5670            \n",
      "                                                   \n",
      "504/504 [==============================]           \n",
      " - 30s 60ms/step - loss: 0.7563 - acc: 0.5774 - val_loss: 0.6645 - val_acc: 0.6614\n",
      "\n",
      "Epoch 3/5                                          \n",
      " 64/504 [==>...........................]           \n",
      " - ETA: 25s - loss: 0.7887 - acc: 0.6094           \n",
      "                                                  \n",
      "128/504 [======>.......................]           \n",
      " - ETA: 24s - loss: 0.7503 - acc: 0.5703           \n",
      "                                                  \n",
      "192/504 [==========>...................]           \n",
      " - ETA: 21s - loss: 0.7680 - acc: 0.5938           \n",
      "                                                  \n",
      "256/504 [==============>...............]           \n",
      " - ETA: 16s - loss: 0.7488 - acc: 0.5859           \n",
      "                                                  \n",
      "320/504 [==================>...........]           \n",
      " - ETA: 11s - loss: 0.7895 - acc: 0.5844           \n",
      "                                                  \n",
      "384/504 [=====================>........]           \n",
      " - ETA: 7s - loss: 0.8293 - acc: 0.5625            \n",
      "                                                   \n",
      "448/504 [=========================>....]           \n",
      " - ETA: 3s - loss: 0.8242 - acc: 0.5759            \n",
      "                                                   \n",
      "504/504 [==============================]           \n",
      " - 32s 64ms/step - loss: 0.8043 - acc: 0.5873 - val_loss: 0.6563 - val_acc: 0.6614\n",
      "\n",
      "Epoch 4/5                                          \n",
      " 64/504 [==>...........................]           \n",
      " - ETA: 24s - loss: 0.7161 - acc: 0.4844           \n",
      "                                                  \n",
      "128/504 [======>.......................]           \n",
      " - ETA: 20s - loss: 0.7428 - acc: 0.5078           \n",
      "                                                  \n",
      "192/504 [==========>...................]           \n",
      " - ETA: 17s - loss: 0.7348 - acc: 0.5365           \n",
      "                                                  \n",
      "256/504 [==============>...............]           \n",
      " - ETA: 13s - loss: 0.7704 - acc: 0.5742           \n",
      "                                                  \n",
      "320/504 [==================>...........]           \n",
      " - ETA: 10s - loss: 0.7741 - acc: 0.5750           \n",
      "                                                  \n",
      "384/504 [=====================>........]           \n",
      " - ETA: 6s - loss: 0.7586 - acc: 0.5964            \n",
      "                                                   \n",
      "448/504 [=========================>....]           \n",
      " - ETA: 3s - loss: 0.7377 - acc: 0.6071            \n",
      "                                                   \n",
      "504/504 [==============================]           \n",
      " - 30s 60ms/step - loss: 0.7422 - acc: 0.6012 - val_loss: 0.6497 - val_acc: 0.6614\n",
      "\n",
      "Epoch 5/5                                          \n",
      " 64/504 [==>...........................]           \n",
      " - ETA: 30s - loss: 0.6714 - acc: 0.6719           \n",
      "                                                  \n",
      "128/504 [======>.......................]           \n",
      " - ETA: 24s - loss: 0.7213 - acc: 0.6484           \n",
      "                                                  \n",
      "192/504 [==========>...................]           \n",
      " - ETA: 18s - loss: 0.7874 - acc: 0.6302           \n",
      "                                                  \n",
      "256/504 [==============>...............]           \n",
      " - ETA: 14s - loss: 0.7853 - acc: 0.6211           \n",
      "                                                  \n",
      "320/504 [==================>...........]           \n",
      " - ETA: 11s - loss: 0.7770 - acc: 0.6406           \n",
      "                                                  \n",
      "384/504 [=====================>........]           \n",
      " - ETA: 7s - loss: 0.7750 - acc: 0.6328            \n",
      "                                                   \n",
      "448/504 [=========================>....]           \n",
      " - ETA: 3s - loss: 0.7541 - acc: 0.6429            \n",
      "                                                   \n",
      "504/504 [==============================]           \n",
      " - 32s 63ms/step - loss: 0.7710 - acc: 0.6389 - val_loss: 0.6459 - val_acc: 0.6614\n",
      "\n",
      "Best validation acc of epoch:                      \n",
      "0.6614173331598597                                 \n",
      "Train on 504 samples, validate on 127 samples                                 \n",
      "Epoch 1/10                                                                    \n",
      " 64/504 [==>...........................]                                      \n",
      " - ETA: 39s - loss: 0.6991 - acc: 0.5156                                      \n",
      "                                                                             \n",
      "128/504 [======>.......................]                                      \n",
      " - ETA: 26s - loss: 0.7113 - acc: 0.4766                                      \n",
      "                                                                             \n",
      "192/504 [==========>...................]                                      \n",
      " - ETA: 20s - loss: 0.7201 - acc: 0.4323                                      \n",
      "                                                                             \n",
      "256/504 [==============>...............]                                      \n",
      " - ETA: 15s - loss: 0.7167 - acc: 0.4336                                      \n",
      "                                                                             \n",
      "320/504 [==================>...........]                                      \n",
      " - ETA: 11s - loss: 0.7139 - acc: 0.4531                                      \n",
      "                                                                             \n",
      "384/504 [=====================>........]                                      \n",
      " - ETA: 7s - loss: 0.7183 - acc: 0.4531                                       \n",
      "                                                                              \n",
      "448/504 [=========================>....]                                      \n",
      " - ETA: 3s - loss: 0.7134 - acc: 0.4665                                       \n",
      "                                                                              \n",
      "504/504 [==============================]                                      \n",
      " - 31s 61ms/step - loss: 0.7109 - acc: 0.4702 - val_loss: 0.6910 - val_acc: 0.6614\n",
      "\n",
      "Epoch 2/10                                                                    \n",
      " 64/504 [==>...........................]                                      \n",
      " - ETA: 22s - loss: 0.6783 - acc: 0.5312                                      \n",
      "                                                                             \n",
      "128/504 [======>.......................]                                      \n",
      " - ETA: 21s - loss: 0.7179 - acc: 0.4766                                      \n",
      "                                                                             \n",
      "192/504 [==========>...................]                                      \n",
      " - ETA: 17s - loss: 0.7029 - acc: 0.5312                                      \n",
      "                                                                             \n",
      "256/504 [==============>...............]                                      \n",
      " - ETA: 13s - loss: 0.6977 - acc: 0.5234                                      \n",
      "                                                                             \n",
      "320/504 [==================>...........]                                      \n",
      " - ETA: 9s - loss: 0.6952 - acc: 0.5188                                       \n",
      "                                                                              \n",
      "384/504 [=====================>........]                                      \n",
      " - ETA: 6s - loss: 0.6951 - acc: 0.5208                                       \n",
      "                                                                              \n",
      "448/504 [=========================>....]                                      \n",
      " - ETA: 2s - loss: 0.7024 - acc: 0.4978                                       \n",
      "                                                                              \n",
      "504/504 [==============================]                                      \n",
      " - 28s 56ms/step - loss: 0.7024 - acc: 0.4921 - val_loss: 0.6891 - val_acc: 0.6614\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10                                                                    \n",
      " 64/504 [==>...........................]                                      \n",
      " - ETA: 22s - loss: 0.7667 - acc: 0.4062                                      \n",
      "                                                                             \n",
      "128/504 [======>.......................]                                      \n",
      " - ETA: 19s - loss: 0.7230 - acc: 0.5156                                      \n",
      "                                                                             \n",
      "192/504 [==========>...................]                                      \n",
      " - ETA: 16s - loss: 0.7116 - acc: 0.5417                                      \n",
      "                                                                             \n",
      "256/504 [==============>...............]                                      \n",
      " - ETA: 13s - loss: 0.7076 - acc: 0.5039                                      \n",
      "                                                                             \n",
      "320/504 [==================>...........]                                      \n",
      " - ETA: 10s - loss: 0.7182 - acc: 0.4844                                      \n",
      "                                                                             \n",
      "384/504 [=====================>........]                                      \n",
      " - ETA: 6s - loss: 0.7116 - acc: 0.4922                                       \n",
      "                                                                              \n",
      "448/504 [=========================>....]                                      \n",
      " - ETA: 3s - loss: 0.7076 - acc: 0.5067                                       \n",
      "                                                                              \n",
      "504/504 [==============================]                                      \n",
      " - 29s 57ms/step - loss: 0.7085 - acc: 0.5099 - val_loss: 0.6874 - val_acc: 0.6614\n",
      "\n",
      "Epoch 4/10                                                                    \n",
      " 64/504 [==>...........................]                                      \n",
      " - ETA: 22s - loss: 0.7392 - acc: 0.5312                                      \n",
      "                                                                             \n",
      "128/504 [======>.......................]                                      \n",
      " - ETA: 19s - loss: 0.7135 - acc: 0.5391                                      \n",
      "                                                                             \n",
      "192/504 [==========>...................]                                      \n",
      " - ETA: 15s - loss: 0.6979 - acc: 0.5312                                      \n",
      "                                                                             \n",
      "256/504 [==============>...............]                                      \n",
      " - ETA: 12s - loss: 0.6998 - acc: 0.5234                                      \n",
      "                                                                             \n",
      "320/504 [==================>...........]                                      \n",
      " - ETA: 9s - loss: 0.6966 - acc: 0.5437                                       \n",
      "                                                                              \n",
      "384/504 [=====================>........]                                      \n",
      " - ETA: 5s - loss: 0.7025 - acc: 0.5365                                       \n",
      "                                                                              \n",
      "448/504 [=========================>....]                                      \n",
      " - ETA: 2s - loss: 0.7008 - acc: 0.5246                                       \n",
      "                                                                              \n",
      "504/504 [==============================]                                      \n",
      " - 27s 54ms/step - loss: 0.6995 - acc: 0.5298 - val_loss: 0.6860 - val_acc: 0.6614\n",
      "\n",
      "Epoch 5/10                                                                    \n",
      " 64/504 [==>...........................]                                      \n",
      " - ETA: 24s - loss: 0.7857 - acc: 0.5469                                      \n",
      "                                                                             \n",
      "128/504 [======>.......................]                                      \n",
      " - ETA: 20s - loss: 0.7363 - acc: 0.5859                                      \n",
      "                                                                             \n",
      "192/504 [==========>...................]                                      \n",
      " - ETA: 16s - loss: 0.7256 - acc: 0.5521                                      \n",
      "                                                                             \n",
      "256/504 [==============>...............]                                      \n",
      " - ETA: 12s - loss: 0.7199 - acc: 0.5469                                      \n",
      "                                                                             \n",
      "320/504 [==================>...........]                                      \n",
      " - ETA: 9s - loss: 0.7137 - acc: 0.5625                                       \n",
      "                                                                              \n",
      "384/504 [=====================>........]                                      \n",
      " - ETA: 6s - loss: 0.7108 - acc: 0.5469                                       \n",
      "                                                                              \n",
      "448/504 [=========================>....]                                      \n",
      " - ETA: 2s - loss: 0.7059 - acc: 0.5670                                       \n",
      "                                                                              \n",
      "504/504 [==============================]                                      \n",
      " - 28s 55ms/step - loss: 0.7072 - acc: 0.5397 - val_loss: 0.6842 - val_acc: 0.6614\n",
      "\n",
      "Epoch 6/10                                                                    \n",
      " 64/504 [==>...........................]                                      \n",
      " - ETA: 26s - loss: 0.6751 - acc: 0.4844                                      \n",
      "                                                                             \n",
      "128/504 [======>.......................]                                      \n",
      " - ETA: 22s - loss: 0.6994 - acc: 0.5000                                      \n",
      "                                                                             \n",
      "192/504 [==========>...................]                                      \n",
      " - ETA: 18s - loss: 0.6967 - acc: 0.5000                                      \n",
      "                                                                             \n",
      "256/504 [==============>...............]                                      \n",
      " - ETA: 14s - loss: 0.6915 - acc: 0.5312                                      \n",
      "                                                                             \n",
      "320/504 [==================>...........]                                      \n",
      " - ETA: 10s - loss: 0.6875 - acc: 0.5437                                      \n",
      "                                                                             \n",
      "384/504 [=====================>........]                                      \n",
      " - ETA: 6s - loss: 0.6886 - acc: 0.5417                                       \n",
      "                                                                              \n",
      "448/504 [=========================>....]                                      \n",
      " - ETA: 3s - loss: 0.6985 - acc: 0.5402                                       \n",
      "                                                                              \n",
      "504/504 [==============================]                                      \n",
      " - 30s 60ms/step - loss: 0.7016 - acc: 0.5417 - val_loss: 0.6825 - val_acc: 0.6614\n",
      "\n",
      "Epoch 7/10                                                                    \n",
      " 64/504 [==>...........................]                                      \n",
      " - ETA: 23s - loss: 0.7041 - acc: 0.5625                                      \n",
      "                                                                             \n",
      "128/504 [======>.......................]                                      \n",
      " - ETA: 19s - loss: 0.6987 - acc: 0.5547                                      \n",
      "                                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/504 [==========>...................]                                      \n",
      " - ETA: 15s - loss: 0.6879 - acc: 0.5625                                      \n",
      "                                                                             \n",
      "256/504 [==============>...............]                                      \n",
      " - ETA: 12s - loss: 0.6846 - acc: 0.5820                                      \n",
      "                                                                             \n",
      "320/504 [==================>...........]                                      \n",
      " - ETA: 9s - loss: 0.6818 - acc: 0.6094                                       \n",
      "                                                                              \n",
      "384/504 [=====================>........]                                      \n",
      " - ETA: 6s - loss: 0.6809 - acc: 0.5964                                       \n",
      "                                                                              \n",
      "448/504 [=========================>....]                                      \n",
      " - ETA: 2s - loss: 0.6901 - acc: 0.6004                                       \n",
      "                                                                              \n",
      "504/504 [==============================]                                      \n",
      " - 28s 55ms/step - loss: 0.6890 - acc: 0.5913 - val_loss: 0.6809 - val_acc: 0.6614\n",
      "\n",
      "Epoch 8/10                                                                    \n",
      " 64/504 [==>...........................]                                      \n",
      " - ETA: 22s - loss: 0.7207 - acc: 0.5938                                      \n",
      "                                                                             \n",
      "128/504 [======>.......................]                                      \n",
      " - ETA: 20s - loss: 0.6956 - acc: 0.6250                                      \n",
      "                                                                             \n",
      "192/504 [==========>...................]                                      \n",
      " - ETA: 16s - loss: 0.6856 - acc: 0.5938                                      \n",
      "                                                                             \n",
      "256/504 [==============>...............]                                      \n",
      " - ETA: 13s - loss: 0.6844 - acc: 0.6172                                      \n",
      "                                                                             \n",
      "320/504 [==================>...........]                                      \n",
      " - ETA: 9s - loss: 0.6854 - acc: 0.6281                                       \n",
      "                                                                              \n",
      "384/504 [=====================>........]                                      \n",
      " - ETA: 6s - loss: 0.6908 - acc: 0.6224                                       \n",
      "                                                                              \n",
      "448/504 [=========================>....]                                      \n",
      " - ETA: 2s - loss: 0.6871 - acc: 0.6272                                       \n",
      "                                                                              \n",
      "504/504 [==============================]                                      \n",
      " - 28s 55ms/step - loss: 0.6881 - acc: 0.6349 - val_loss: 0.6794 - val_acc: 0.6614\n",
      "\n",
      "Epoch 9/10                                                                    \n",
      " 64/504 [==>...........................]                                      \n",
      " - ETA: 21s - loss: 0.6886 - acc: 0.6719                                      \n",
      "                                                                             \n",
      "128/504 [======>.......................]                                      \n",
      " - ETA: 18s - loss: 0.6833 - acc: 0.6484                                      \n",
      "                                                                             \n",
      "192/504 [==========>...................]                                      \n",
      " - ETA: 15s - loss: 0.6746 - acc: 0.6562                                      \n",
      "                                                                             \n",
      "256/504 [==============>...............]                                      \n",
      " - ETA: 12s - loss: 0.6744 - acc: 0.6562                                      \n",
      "                                                                             \n",
      "320/504 [==================>...........]                                      \n",
      " - ETA: 9s - loss: 0.6756 - acc: 0.6562                                       \n",
      "                                                                              \n",
      "384/504 [=====================>........]                                      \n",
      " - ETA: 6s - loss: 0.6757 - acc: 0.6380                                       \n",
      "                                                                              \n",
      "448/504 [=========================>....]                                      \n",
      " - ETA: 2s - loss: 0.6748 - acc: 0.6451                                       \n",
      "                                                                              \n",
      "504/504 [==============================]                                      \n",
      " - 27s 54ms/step - loss: 0.6780 - acc: 0.6488 - val_loss: 0.6781 - val_acc: 0.6614\n",
      "\n",
      "Epoch 10/10                                                                   \n",
      " 64/504 [==>...........................]                                      \n",
      " - ETA: 22s - loss: 0.6683 - acc: 0.6875                                      \n",
      "                                                                             \n",
      "128/504 [======>.......................]                                      \n",
      " - ETA: 19s - loss: 0.6622 - acc: 0.7109                                      \n",
      "                                                                             \n",
      "192/504 [==========>...................]                                      \n",
      " - ETA: 16s - loss: 0.6617 - acc: 0.6927                                      \n",
      "                                                                             \n",
      "256/504 [==============>...............]                                      \n",
      " - ETA: 12s - loss: 0.6637 - acc: 0.6914                                      \n",
      "                                                                             \n",
      "320/504 [==================>...........]                                      \n",
      " - ETA: 9s - loss: 0.6698 - acc: 0.6906                                       \n",
      "                                                                              \n",
      "384/504 [=====================>........]                                      \n",
      " - ETA: 6s - loss: 0.6761 - acc: 0.6849                                       \n",
      "                                                                              \n",
      "448/504 [=========================>....]                                      \n",
      " - ETA: 2s - loss: 0.6786 - acc: 0.6920                                       \n",
      "                                                                              \n",
      "504/504 [==============================]                                      \n",
      " - 29s 58ms/step - loss: 0.6777 - acc: 0.6925 - val_loss: 0.6768 - val_acc: 0.6614\n",
      "\n",
      "Best validation acc of epoch:                                                 \n",
      "0.6614173331598597                                                            \n",
      "Train on 504 samples, validate on 127 samples                                 \n",
      "Epoch 1/15                                                                    \n",
      " 32/504 [>.............................]                                      \n",
      " - ETA: 1:40 - loss: 0.7426 - acc: 0.7500                                     \n",
      "                                                                              \n",
      " 64/504 [==>...........................]                                      \n",
      " - ETA: 1:19 - loss: 0.7320 - acc: 0.6406                                     \n",
      "                                                                              \n",
      " 96/504 [====>.........................]                                      \n",
      " - ETA: 1:09 - loss: 0.7163 - acc: 0.6146                                     \n",
      "                                                                              \n",
      "128/504 [======>.......................]                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - ETA: 58s - loss: 0.7206 - acc: 0.6172                                      \n",
      "                                                                             \n",
      "160/504 [========>.....................]                                      \n",
      " - ETA: 51s - loss: 0.7384 - acc: 0.6000                                      \n",
      "                                                                             \n",
      "192/504 [==========>...................]                                      \n",
      " - ETA: 45s - loss: 0.7217 - acc: 0.6198                                      \n",
      "                                                                             \n",
      "224/504 [============>.................]                                      \n",
      " - ETA: 40s - loss: 0.7079 - acc: 0.6518                                      \n",
      "                                                                             \n",
      "256/504 [==============>...............]                                      \n",
      " - ETA: 35s - loss: 0.7034 - acc: 0.6406                                      \n",
      "                                                                             \n",
      "288/504 [================>.............]                                      \n",
      " - ETA: 30s - loss: 0.7032 - acc: 0.6424                                      \n",
      " 40%|████      | 2/5 [08:14<10:00, 200.22s/it, best loss: -0.6614173331598597]"
     ]
    }
   ],
   "source": [
    "# Tuning hyperparameter with Hyperas\n",
    "# Code source: https://github.com/maxpumperla/hyperas\n",
    "\n",
    "# for RNN\n",
    "\n",
    "def data():\n",
    "    \"\"\"\n",
    "    Data providing function:\n",
    "\n",
    "    This function is separated from create_model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    \"\"\"\n",
    "    tf_vectors = np.load(\"saved_csv/tf_vectors.npy\")\n",
    "    \n",
    "    table = pd.read_csv(\"saved_csv/table.csv\")\n",
    "    table.drop(columns = \"Unnamed: 0\",inplace=True)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(tf_vectors,table.iloc[:,1].values,test_size = 0.2)\n",
    "\n",
    "    x_train = x_train.reshape(631,1130,1)\n",
    "    y_train = y_train.reshape(631,1)\n",
    "    x_test = x_test.reshape(158,1130,1)\n",
    "    y_test = y_test.reshape(158,1)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM({{choice([32,64,96,128])}},activation={{choice([\"relu\",\"elu\"])}}, \n",
    "                   input_shape = (x_train.shape[1:]), return_sequences=True, dropout={{uniform(0,1)}}))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM({{choice([32,64,96,128])}}, activation={{choice([\"relu\",\"elu\"])}}, dropout={{uniform(0,1)}}))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense({{choice([32,64,96,128])}}, activation={{choice([\"relu\",\"elu\"])}}))\n",
    "    model.add(Dropout({{uniform(0,1)}}))\n",
    "\n",
    "    model.add(Dense(2, activation={{choice([\"softmax\",\"sigmoid\"])}}))\n",
    "\n",
    "    # setting up optimizer hyperparameters\n",
    "    sgd = SGD(lr={{uniform(0,0.01)}},decay=0.0, momentum = 0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer = sgd, metrics = [\"accuracy\"])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=2, verbose=1)\n",
    "\n",
    "    result = model.fit(x_train,y_train, batch_size = {{choice([16, 32, 64])}}, epochs = {{choice([5, 10, 15])}}, \n",
    "                       callbacks = [es], validation_split=0.2)\n",
    "\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    best_run, best_model = optim.minimize(model=create_model,data=data,algo=tpe.suggest,max_evals=5,trials=Trials(),\n",
    "                                          notebook_name='Capstone modelling stage v.3-RNN')\n",
    "    X_train, Y_train, X_test, Y_test = data()\n",
    "    print(\"Evaluation of best performing model:\")\n",
    "    print(best_model.evaluate(X_test, Y_test))\n",
    "    print(\"Best performing model chosen hyper-parameters:\")\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using results from Hyperas to create the model\n",
    "\n",
    "def RNN_model(X,y):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(32,activation=\"elu\", input_shape = (X.shape[1:]), return_sequences=True, dropout=0.3207527760045966))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(96, activation=\"elu\", dropout=0.7342146978592597))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(32, activation='elu'))\n",
    "    model.add(Dropout(0.692539034315719))\n",
    "\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    # setting up SGD (optimizer) hyperparameters\n",
    "    sgd = SGD(lr=0.004371162594318422,decay=0.0, momentum = 0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer = sgd, metrics = [\"accuracy\"])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=2, verbose=1)\n",
    "\n",
    "    result = model.fit(X,y, batch_size = 64, epochs = 5, callbacks = [es], validation_split=0.2)\n",
    "    \n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 631 samples, validate on 158 samples\n",
      "Epoch 1/5\n",
      "631/631 [==============================] - 41s 65ms/step - loss: 0.7843 - acc: 0.5357 - val_loss: 0.6930 - val_acc: 0.5696\n",
      "Epoch 2/5\n",
      "631/631 [==============================] - 36s 58ms/step - loss: 0.7421 - acc: 0.5309 - val_loss: 0.6930 - val_acc: 0.5759\n",
      "Epoch 3/5\n",
      "631/631 [==============================] - 37s 58ms/step - loss: 0.8069 - acc: 0.4723 - val_loss: 0.6932 - val_acc: 0.4304\n",
      "Epoch 00003: early stopping\n"
     ]
    }
   ],
   "source": [
    "X = tf_vectors\n",
    "y = table.iloc[:,1].values\n",
    "\n",
    "X = X.toarray().reshape(789,1130,1)\n",
    "y = y.reshape(789,1)\n",
    "\n",
    "model, result = RNN_model(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Briefly describe what you think the tech industry as a whole and/or employers could do to improve mental health support for employees.they hate me\n",
      "Processing...\n",
      "Almost there...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.4998527 , 0.50014734]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input an response\n",
    "response = input(\"Briefly describe what you think the tech industry as a whole and/or \\\n",
    "employers could do to improve mental health support for employees.\")\n",
    "\n",
    "print(\"Processing...\")\n",
    "\n",
    "# Text processing to prepare data for RNN\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "sentences = []\n",
    "doc = nlp(response)\n",
    "\n",
    "sentence = []\n",
    "for token in doc:\n",
    "    sentence.append(token.lemma_)\n",
    "\n",
    "sentences.append(\" \".join(sentence))\n",
    "\n",
    "print(\"Almost there...\")\n",
    "\n",
    "# Processing text with TfidfVectorizer\n",
    "tf_vectors = tf_model.transform(sentences)\n",
    "\n",
    "# predicting the result using the model\n",
    "X_test = tf_vectors.toarray().reshape(1,1130,1)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# printing the result\n",
    "if y_pred[0][0] > 0.5:\n",
    "    print(\"The model predicts you are hesitant with discussing MH issue with your coworkers.\")\n",
    "else:\n",
    "    print(\"The model predicts you to have comfortable with discussing MH issue with your coworkers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CNN\n",
    "def data():\n",
    "    \"\"\"\n",
    "    Data providing function:\n",
    "\n",
    "    This function is separated from create_model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    \"\"\"\n",
    "    tf_vectors = np.load(\"saved_csv/tf_vectors.npy\")\n",
    "    \n",
    "    table = pd.read_csv(\"saved_csv/table.csv\")\n",
    "    table.drop(columns = \"Unnamed: 0\",inplace=True)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(tf_vectors,table.iloc[:,1].values,test_size = 0.2)\n",
    "\n",
    "    x_train = x_train.reshape(631,1127,1)\n",
    "    y_train = y_train.reshape(631,1)\n",
    "    x_test = x_test.reshape(158,1127,1)\n",
    "    y_test = y_test.reshape(158,1)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D({{choice([32,64,96,128])}},{{choice([5,10,15,20])}},activation={{choice([\"relu\",\"elu\"])}}, \n",
    "                   input_shape = (x_train.shape[1:])))\n",
    "    model.add(Conv1D({{choice([32,64,96,128])}},{{choice([5,10,15,20])}},activation={{choice([\"relu\",\"elu\"])}}))\n",
    "    model.add(MaxPooling1D({{choice([1,2,3,4,5,6])}}))\n",
    "\n",
    "    model.add(Conv1D({{choice([32,64,96,128])}},{{choice([5,10,15,20])}},activation={{choice([\"relu\",\"elu\"])}}))\n",
    "    model.add(Conv1D({{choice([32,64,96,128])}},{{choice([5,10,15,20])}},activation={{choice([\"relu\",\"elu\"])}}))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "              \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense({{choice([32,64,96,128])}},activation={{choice([\"relu\",\"elu\"])}}))\n",
    "    model.add(Dropout({{uniform(0,1)}}))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    sgd = SGD(lr={{uniform(0,0.01)}},decay=0.0, momentum = 0.0, nesterov=False, clipnorm=2.0)\n",
    "              \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', min_delta=0.0001, mode='min', patience=2, verbose=1)\n",
    "\n",
    "    result = model.fit(x_train,y_train, batch_size = {{choice([16, 32, 64])}}, epochs = {{choice([5, 10, 15])}}, \n",
    "                       validation_split = 0.2, callbacks=[early_stop])\n",
    "\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    best_run, best_model = optim.minimize(model=create_model,data=data,algo=tpe.suggest,max_evals=5,trials=Trials(),\n",
    "                                          notebook_name='Capstone modelling stage v.3-RNN')\n",
    "    X_train, Y_train, X_test, Y_test = data()\n",
    "    print(\"Evaluation of best performing model:\")\n",
    "    print(best_model.evaluate(X_test, Y_test))\n",
    "    print(\"Best performing model chosen hyper-parameters:\")\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using results from Hyperas to create the model\n",
    "def CNN_model(x_train,y_train):\n",
    "    CNN_model = Sequential()\n",
    "\n",
    "    CNN_model.add(Conv1D(96,10,activation=\"relu\",input_shape = (x_train.shape[1:])))\n",
    "    CNN_model.add(Conv1D(64,20,activation=\"relu\"))\n",
    "    CNN_model.add(MaxPooling1D(0))\n",
    "\n",
    "    CNN_model.add(Conv1D(64,20,activation=\"elu\"))\n",
    "    CNN_model.add(Conv1D(64,10,activation=\"relu\"))\n",
    "    CNN_model.add(GlobalAveragePooling1D())\n",
    "\n",
    "    CNN_model.add(Flatten())\n",
    "    CNN_model.add(Dense(96,activation=\"elu\"))\n",
    "    CNN_model.add(Dropout(0.9912013870496312))\n",
    "    CNN_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=0.00026079803111884515,decay=0.0, momentum = 0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "    CNN_model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', min_delta=0.0001, mode='min', patience=2, verbose=1)\n",
    "\n",
    "    results = CNN_model.fit(x_train,y_train, batch_size=32, epochs=10, validation_split = 0.2, callbacks=[early_stop])\n",
    "    \n",
    "    return CNN_model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tf_vectors,table.iloc[:,1].values,test_size = 0.2)\n",
    "\n",
    "x_train = x_train.toarray().reshape(631,1127,1)\n",
    "y_train = y_train.reshape(631,1)\n",
    "x_test = x_test.toarray().reshape(158,1127,1)\n",
    "y_test = y_test.reshape(158,1)\n",
    "\n",
    "CNN_model, results = CNN_model(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking test accuracy\n",
    "_, test_acc = CNN_model.evaluate(x_test, y_test, verbose=0)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(64,activation=\"relu\", input_shape = (3,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    # setting up SGD (optimizer) hyperparameters\n",
    "    sgd = SGD(lr=0.0001, decay=0.0, momentum = 0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer = sgd, metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tf_vectors.toarray(),table.iloc[:,1].values,test_size = 0.8)\n",
    "\n",
    "base_models = [KNeighborsClassifier(n_neighbors=1),\n",
    "               LogisticRegression(),\n",
    "               XGBClassifier()]\n",
    "\n",
    "base_models = [(f'{model.__class__.__name__}-{i}', model) for i, model in enumerate(base_models)]\n",
    "\n",
    "stacked_model = StackingCVClassifier(classifiers=[model for _, model in base_models],\n",
    "                                   meta_classifier=KerasClassifier(build_fn=NN_model, batch_size = 16, epochs = 5, validation_split = 0.2), \n",
    "                                   use_features_in_secondary=False)\n",
    "\n",
    "params = {'kneighborsclassifier__n_neighbors': [5,10,15,20],'kneighborsclassifier__n_jobs': [6],\n",
    "          'xgbclassifier__max_depth' : [1,2,3],'xgbclassifier__n_estimators' : [50,100,150],'xgbclassifier__n_jobs': [6],\n",
    "          'logisticregression__penalty': ['l1','l2'],'logisticregression__C': [0.0001,0.01,1,10],'logisticregression__n_jobs': [6]}\n",
    "\n",
    "grid = GridSearchCV(estimator=stacked_model, param_grid=params, cv=3,refit=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customization stopwords to filter out some words\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"mental\",\"health\",\"issue\",\"work\",\n",
    "                  \"take\",\"hour\",\"tech\",\"industry\",\"people\",\"employee\"])\n",
    "\n",
    "\n",
    "for num in range(2):\n",
    "\n",
    "    classes = table[table.iloc[:,-1]==num]\n",
    "\n",
    "    # CountVectorizer with SpaCy Lemmatization\n",
    "    spp = SpacyPipeProcessor(nlp, n_threads=1, multi_iters=True)\n",
    "    spacy_docs = spp(classes.iloc[:,0]);\n",
    "\n",
    "    slcv = SpacyLemmaCountVectorizer(min_df=3,stop_words=stopwords, ngram_range=(1, 3), ignore_chars='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "    slcv.fit(spacy_docs)\n",
    "    count_vectors = slcv.transform(spacy_docs); count_vectors\n",
    "\n",
    "    # Pulling out the list of parsed words and put them into a wordcloud\n",
    "    list_of_words = slcv.vocabulary_.keys()\n",
    "    list_of_words = list(list_of_words)\n",
    "    list_of_words.sort()\n",
    "\n",
    "    wordcloud = WordCloud(background_color=\"white\").generate(\" \".join(list_of_words))\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
