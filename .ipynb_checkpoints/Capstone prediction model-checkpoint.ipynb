{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project\n",
    "-------\n",
    "\n",
    "### Stage 2 - Modelling phase\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages and data\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nattiechan/anaconda3/envs/myenv/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "\n",
    "# Basics\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Graphs\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Preprocessing; model selection and evaluation\n",
    "from sklearn import pipeline, preprocessing\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# text handling\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import statsmodels.api as sm\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# for custom countvectorizer with SpaCy lemmatization\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, VectorizerMixin\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# WordCloud\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "data = pd.read_csv(\"saved_csv/df.csv\")\n",
    "data.drop(columns = \"Unnamed: 0\",inplace=True)\n",
    "\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "### Can we predict one's comfort level in discussing MH at workplace using participants' qualitative responses of ways to improve MH support?\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the responses as independent variables\n",
    "corpus = df.iloc[:,-9]\n",
    "\n",
    "# Dependent variables\n",
    "question = \"Would you feel comfortable discussing a mental health issue with your coworkers?\"\n",
    "\n",
    "answers = [\"Maybe\",\"No\",\"Not Applicable\",\"Yes\"]\n",
    "\n",
    "dep = df[question].copy()\n",
    "\n",
    "for num in range(len(answers)):\n",
    "    if num != 3:\n",
    "        dep[dep==answers[num]] = 0 #Hesitant\n",
    "    else:\n",
    "        dep[dep==answers[num]] = 1 #Comfortable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a table with both independent and dependent variables\n",
    "table = pd.concat([corpus,dep],axis=1)\n",
    "\n",
    "# dropping columns that did not answer the question\n",
    "index = table[table.iloc[:,0]==\"Did not answer\"].index\n",
    "table.drop(index,axis=0,inplace=True)\n",
    "\n",
    "# resetting the index\n",
    "table = table.reset_index()\n",
    "table.drop(\"index\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training/test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(table.iloc[:,0].values,table.iloc[:,1].values,\n",
    "                                                    test_size = 0.2, stratify=table.iloc[:,1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<631x930 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9438 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process and transform x_train\n",
    "\n",
    "# Lemmatization using SpaCy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# customizing stopwords to exclude certain stopwords\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "words = [\"against\",\"all\",\"aren't\",\"can't\",\"can\",\"cannot\",\"could\",\"couldn't\",\"did\",\n",
    "         \"didn't\",\"doing\",\"don't\",\"hasn't\",\"hadn't\",\"ever\",\"few\",\"mustn't\",\"once\",\"shan't\"]\n",
    "\n",
    "for word in words:\n",
    "    stopwords.remove(word)\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for num in range(len(x_train)):\n",
    "    doc = nlp(x_train[num])\n",
    "\n",
    "    sentence = []\n",
    "    for token in doc:\n",
    "        sentence.append(token.lemma_)\n",
    "\n",
    "    sentences.append(\" \".join(sentence))\n",
    "\n",
    "# Processing text with TfidfVectorizer\n",
    "tf_model = TfidfVectorizer(stop_words=stopwords,ngram_range=(1,3), min_df=3)\n",
    "tf_vectors = tf_model.fit_transform(sentences); tf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling with SMOTE\n",
    "sm = SMOTE(n_jobs = 6)\n",
    "X_res,y_res = sm.fit_resample(tf_vectors.toarray(),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<158x930 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1967 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process and transform x_test\n",
    "sentences = []\n",
    "\n",
    "for num in range(len(x_test)):\n",
    "    doc = nlp(x_test[num])\n",
    "\n",
    "    sentence = []\n",
    "    for token in doc:\n",
    "        sentence.append(token.lemma_)\n",
    "\n",
    "    sentences.append(\" \".join(sentence))\n",
    "\n",
    "x_test_vectors = tf_model.transform(sentences); x_test_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model using Stacking CV Classifier\n",
    "base_models = [SVC(C=1.0, kernel=\"rbf\", class_weight = {0: 1, 1: 2}), \n",
    "               LogisticRegression(penalty = \"l1\", class_weight = {0: 1, 1: 2})]\n",
    "\n",
    "base_models = [(f'{model.__class__.__name__}-{i}', model) for i, model in enumerate(base_models)]\n",
    "\n",
    "stacked_model = StackingCVClassifier(classifiers=[model for _, model in base_models],\n",
    "                                     meta_classifier=GradientBoostingClassifier(n_estimators = 100, max_depth = 3), \n",
    "                                     use_features_in_secondary=False)\n",
    "\n",
    "model = stacked_model.fit(X_res, y_res.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4578313253012048"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test_vectors.toarray())\n",
    "\n",
    "f1_score(y_test.astype(int),y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input an response and see if the model predicts correctly\n",
    "response = input(\"Briefly describe what you think the tech industry as a whole and/or \\\n",
    "employers could do to improve mental health support for employees.\")\n",
    "\n",
    "print(\"Processing...\")\n",
    "\n",
    "# Text processing to prepare data for RNN\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "sentences = []\n",
    "doc = nlp(response)\n",
    "\n",
    "sentence = []\n",
    "for token in doc:\n",
    "    sentence.append(token.lemma_)\n",
    "\n",
    "sentences.append(\" \".join(sentence))\n",
    "\n",
    "print(\"Almost there...\")\n",
    "\n",
    "# Processing text with TfidfVectorizer\n",
    "tf_vectors = tf_model.transform(sentences)\n",
    "\n",
    "# predicting the result using the model\n",
    "y_pred = model.predict(tf_vectors.toarray())\n",
    "\n",
    "# printing the result\n",
    "if y_pred == 0 :\n",
    "    print(\"The model predicts you are hesitant with discussing MH issue with your coworkers.\")\n",
    "else:\n",
    "    print(\"The model predicts you to have comfortable with discussing MH issue with your coworkers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
