{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project\n",
    "-------\n",
    "\n",
    "### Stage 2 - Modelling phase - Neural Networks\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages and data\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible packages that need to be installed:\n",
    "\n",
    "1. Hyperas\n",
    "\n",
    "<code> conda install -c jaikumarm hyperas </code>\n",
    "\n",
    "2. mlxtend\n",
    "\n",
    "<code> conda install -c conda-forge mlxtend </code>\n",
    "\n",
    "These packages are from the previous notebook. If virtual environments are used for neural network, however, the following packages will need to be installed in order for the notebook to run properly.\n",
    "\n",
    "3. SpaCy\n",
    "\n",
    "<code> conda install -c spacy spacy </code>\n",
    "\n",
    "4. 'en_core_web_md' - library used in SpaCy\n",
    "\n",
    "<code> python -m spacy download en_core_web_md </code>\n",
    "\n",
    "5. wordcloud\n",
    "\n",
    "<code> conda install -c conda-forge wordcloud </code>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "# Hyperas/TensorFlow\n",
    "# the __future__ import command must be in the beginning of the notebook\n",
    "from __future__ import print_function\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from tensorflow.python.keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Basics\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import numpy as np\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Preprocessing; model selection and evaluation\n",
    "from sklearn import pipeline, preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Modelling\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# text handling\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# for custom countvectorizer with SpaCy lemmatization\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, VectorizerMixin\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# WordCloud\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "If we need to move virtual ENV to use Tensorflow we will need to install:\n",
    "\n",
    "1. spacy\n",
    "\n",
    "<code> conda install -c spacy spacy </code>\n",
    "\n",
    "2. 'en_core_web_md'\n",
    "\n",
    "<code> python -m spacy download en_core_web_md </code>\n",
    "\n",
    "3. wordcloud\n",
    "\n",
    "<code> conda install -c conda-forge wordcloud </code>\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "data = pd.read_csv(\"saved_csv/df.csv\")\n",
    "data.drop(columns = \"Unnamed: 0\",inplace=True)\n",
    "\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model to predict comfort level using text responses\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the responses as independent variables\n",
    "corpus = df.iloc[:,-9]\n",
    "\n",
    "# grabbing the dependent variables\n",
    "dependent_class = pd.read_csv(\"saved_csv/q1_dependent_alt.csv\")\n",
    "dependent_class.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "dependent_class_alt = dependent_class.copy()\n",
    "\n",
    "dependent_class_alt[dependent_class_alt < 2] = 0\n",
    "dependent_class_alt[dependent_class_alt >= 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a table with both independent and dependent variables\n",
    "table = pd.concat([corpus,dependent_class_alt],axis=1)\n",
    "\n",
    "# dropping columns that did not answer the question\n",
    "index = table[table.iloc[:,0]==\"Did not answer\"].index\n",
    "\n",
    "table.drop(index,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Text processing to prepare data for RNN\n",
    "\n",
    "# Lemmatization using SpaCy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for num in range(len(table)):\n",
    "    doc = nlp(table.iloc[num,0])\n",
    "\n",
    "    sentence = []\n",
    "    for token in doc:\n",
    "        sentence.append(token.lemma_)\n",
    "\n",
    "    sentences.append(\" \".join(sentence))\n",
    "\n",
    "# Processing text with TfidfVectorizer\n",
    "tf_model = TfidfVectorizer(stop_words=STOPWORDS,ngram_range=(1,3), min_df=3)\n",
    "tf_vectors = tf_model.fit_transform(sentences); tf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving files to be loaded in Hyperas functions\n",
    "np.save(\"saved_csv/tf_vectors.npy\", tf_vectors.toarray(), allow_pickle=True, fix_imports=True)\n",
    "\n",
    "table.to_csv(\"saved_csv/table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning hyperparameter with Hyperas\n",
    "# Code source: https://github.com/maxpumperla/hyperas\n",
    "\n",
    "# for RNN\n",
    "\n",
    "def data():\n",
    "    \"\"\"\n",
    "    Data providing function:\n",
    "\n",
    "    This function is separated from create_model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    \"\"\"\n",
    "    tf_vectors = np.load(\"saved_csv/tf_vectors.npy\")\n",
    "    \n",
    "    table = pd.read_csv(\"saved_csv/table.csv\")\n",
    "    table.drop(columns = \"Unnamed: 0\",inplace=True)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(tf_vectors,table.iloc[:,1].values,test_size = 0.2)\n",
    "\n",
    "    x_train = x_train.reshape(631,1127,1)\n",
    "    y_train = y_train.reshape(631,1)\n",
    "    x_test = x_test.reshape(158,1127,1)\n",
    "    y_test = y_test.reshape(158,1)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM({{choice([32,64,96,128])}},activation={{choice([\"relu\",\"elu\"])}}, \n",
    "                   input_shape = (x_train.shape[1:]), return_sequences=True, dropout={{uniform(0,1)}}))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM({{choice([32,64,96,128])}}, activation={{choice([\"relu\",\"elu\"])}}, dropout={{uniform(0,1)}}))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense({{choice([32,64,96,128])}}, activation={{choice([\"relu\",\"elu\"])}}))\n",
    "    model.add(Dropout({{uniform(0,1)}}))\n",
    "\n",
    "    model.add(Dense(2, activation={{choice([\"softmax\",\"sigmoid\"])}}))\n",
    "\n",
    "    # setting up optimizer hyperparameters\n",
    "    sgd = SGD(lr={{uniform(0,0.01)}},decay=0.0, momentum = 0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer = sgd, metrics = [\"accuracy\"])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode=\"min\", patience=2, verbose=1)\n",
    "\n",
    "    result = model.fit(x_train,y_train, batch_size = {{choice([16, 32, 64])}}, epochs = {{choice([5, 10, 15])}}, \n",
    "                       callbacks = [es], validation_split=0.2)\n",
    "\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    best_run, best_model = optim.minimize(model=create_model,data=data,algo=tpe.suggest,max_evals=5,trials=Trials(),\n",
    "                                          notebook_name='Capstone modelling stage v.3-RNN')\n",
    "    X_train, Y_train, X_test, Y_test = data()\n",
    "    print(\"Evaluation of best performing model:\")\n",
    "    print(best_model.evaluate(X_test, Y_test))\n",
    "    print(\"Best performing model chosen hyper-parameters:\")\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using results from Hyperas to create the model\n",
    "\n",
    "def RNN_model(X_train,y_train):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(32,activation=\"elu\", input_shape = (X_train.shape[1:]), return_sequences=True, dropout=0.3207527760045966))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(96, activation=\"elu\", dropout=0.7342146978592597))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(32, activation='elu'))\n",
    "    model.add(Dropout(0.692539034315719))\n",
    "\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    # setting up SGD (optimizer) hyperparameters\n",
    "    sgd = SGD(lr=0.004371162594318422,decay=0.0, momentum = 0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer = sgd, metrics = [\"accuracy\"])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=2, verbose=1)\n",
    "\n",
    "    result = model.fit(X_train,y_train, batch_size = 64, epochs = 5, callbacks = [es], validation_split=0.2)\n",
    "    \n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tf_vectors,table.iloc[:,1].values,test_size = 0.2)\n",
    "\n",
    "X_train = X_train.toarray().reshape(631,1127,1)\n",
    "y_train = y_train.reshape(631,1)\n",
    "X_test = X_test.toarray().reshape(158,1127,1)\n",
    "y_test = y_test.reshape(158,1)\n",
    "\n",
    "model, result = RNN_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking test accuracy\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CNN\n",
    "def data():\n",
    "    \"\"\"\n",
    "    Data providing function:\n",
    "\n",
    "    This function is separated from create_model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    \"\"\"\n",
    "    tf_vectors = np.load(\"saved_csv/tf_vectors.npy\")\n",
    "    \n",
    "    table = pd.read_csv(\"saved_csv/table.csv\")\n",
    "    table.drop(columns = \"Unnamed: 0\",inplace=True)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(tf_vectors,table.iloc[:,1].values,test_size = 0.2)\n",
    "\n",
    "    x_train = x_train.reshape(631,1127,1)\n",
    "    y_train = y_train.reshape(631,1)\n",
    "    x_test = x_test.reshape(158,1127,1)\n",
    "    y_test = y_test.reshape(158,1)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D({{choice([32,64,96,128])}},{{choice([5,10,15,20])}},activation={{choice([\"relu\",\"elu\"])}}, \n",
    "                   input_shape = (x_train.shape[1:])))\n",
    "    model.add(Conv1D({{choice([32,64,96,128])}},{{choice([5,10,15,20])}},activation={{choice([\"relu\",\"elu\"])}}))\n",
    "    model.add(MaxPooling1D({{choice([1,2,3,4,5,6])}}))\n",
    "\n",
    "    model.add(Conv1D({{choice([32,64,96,128])}},{{choice([5,10,15,20])}},activation={{choice([\"relu\",\"elu\"])}}))\n",
    "    model.add(Conv1D({{choice([32,64,96,128])}},{{choice([5,10,15,20])}},activation={{choice([\"relu\",\"elu\"])}}))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "              \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense({{choice([32,64,96,128])}},activation={{choice([\"relu\",\"elu\"])}}))\n",
    "    model.add(Dropout({{uniform(0,1)}}))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    sgd = SGD(lr={{uniform(0,0.01)}},decay=0.0, momentum = 0.0, nesterov=False, clipnorm=2.0)\n",
    "              \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', min_delta=0.0001, mode='min', patience=2, verbose=1)\n",
    "\n",
    "    result = model.fit(x_train,y_train, batch_size = {{choice([16, 32, 64])}}, epochs = {{choice([5, 10, 15])}}, \n",
    "                       validation_split = 0.2, callbacks=[early_stop])\n",
    "\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    best_run, best_model = optim.minimize(model=create_model,data=data,algo=tpe.suggest,max_evals=5,trials=Trials(),\n",
    "                                          notebook_name='Capstone modelling stage v.3-RNN')\n",
    "    X_train, Y_train, X_test, Y_test = data()\n",
    "    print(\"Evaluation of best performing model:\")\n",
    "    print(best_model.evaluate(X_test, Y_test))\n",
    "    print(\"Best performing model chosen hyper-parameters:\")\n",
    "    print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using results from Hyperas to create the model\n",
    "def CNN_model(x_train,y_train):\n",
    "    CNN_model = Sequential()\n",
    "\n",
    "    CNN_model.add(Conv1D(96,10,activation=\"relu\",input_shape = (x_train.shape[1:])))\n",
    "    CNN_model.add(Conv1D(64,20,activation=\"relu\"))\n",
    "    CNN_model.add(MaxPooling1D(0))\n",
    "\n",
    "    CNN_model.add(Conv1D(64,20,activation=\"elu\"))\n",
    "    CNN_model.add(Conv1D(64,10,activation=\"relu\"))\n",
    "    CNN_model.add(GlobalAveragePooling1D())\n",
    "\n",
    "    CNN_model.add(Flatten())\n",
    "    CNN_model.add(Dense(96,activation=\"elu\"))\n",
    "    CNN_model.add(Dropout(0.9912013870496312))\n",
    "    CNN_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=0.00026079803111884515,decay=0.0, momentum = 0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "    CNN_model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', min_delta=0.0001, mode='min', patience=2, verbose=1)\n",
    "\n",
    "    results = CNN_model.fit(x_train,y_train, batch_size=32, epochs=10, validation_split = 0.2, callbacks=[early_stop])\n",
    "    \n",
    "    return CNN_model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tf_vectors,table.iloc[:,1].values,test_size = 0.2)\n",
    "\n",
    "x_train = x_train.toarray().reshape(631,1127,1)\n",
    "y_train = y_train.reshape(631,1)\n",
    "x_test = x_test.toarray().reshape(158,1127,1)\n",
    "y_test = y_test.reshape(158,1)\n",
    "\n",
    "CNN_model, results = CNN_model(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking test accuracy\n",
    "_, test_acc = CNN_model.evaluate(x_test, y_test, verbose=0)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(64,activation=\"relu\", input_shape = (3,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    # setting up SGD (optimizer) hyperparameters\n",
    "    sgd = SGD(lr=0.0001, decay=0.0, momentum = 0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer = sgd, metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tf_vectors.toarray(),table.iloc[:,1].values,test_size = 0.8)\n",
    "\n",
    "base_models = [KNeighborsClassifier(n_neighbors=1),\n",
    "               LogisticRegression(),\n",
    "               XGBClassifier()]\n",
    "\n",
    "base_models = [(f'{model.__class__.__name__}-{i}', model) for i, model in enumerate(base_models)]\n",
    "\n",
    "stacked_model = StackingCVClassifier(classifiers=[model for _, model in base_models],\n",
    "                                   meta_classifier=KerasClassifier(build_fn=NN_model, batch_size = 16, epochs = 5, validation_split = 0.2), \n",
    "                                   use_features_in_secondary=False)\n",
    "\n",
    "params = {'kneighborsclassifier__n_neighbors': [5,10,15,20],'kneighborsclassifier__n_jobs': [6],\n",
    "          'xgbclassifier__max_depth' : [1,2,3],'xgbclassifier__n_estimators' : [50,100,150],'xgbclassifier__n_jobs': [6],\n",
    "          'logisticregression__penalty': ['l1','l2'],'logisticregression__C': [0.0001,0.01,1,10],'logisticregression__n_jobs': [6]}\n",
    "\n",
    "grid = GridSearchCV(estimator=stacked_model, param_grid=params, cv=3,refit=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customization stopwords to filter out some words\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"mental\",\"health\",\"issue\",\"work\",\n",
    "                  \"take\",\"hour\",\"tech\",\"industry\",\"people\",\"employee\"])\n",
    "\n",
    "\n",
    "for num in range(2):\n",
    "\n",
    "    classes = table[table.iloc[:,-1]==num]\n",
    "\n",
    "    # CountVectorizer with SpaCy Lemmatization\n",
    "    spp = SpacyPipeProcessor(nlp, n_threads=1, multi_iters=True)\n",
    "    spacy_docs = spp(classes.iloc[:,0]);\n",
    "\n",
    "    slcv = SpacyLemmaCountVectorizer(min_df=3,stop_words=stopwords, ngram_range=(1, 3), ignore_chars='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "    slcv.fit(spacy_docs)\n",
    "    count_vectors = slcv.transform(spacy_docs); count_vectors\n",
    "\n",
    "    # Pulling out the list of parsed words and put them into a wordcloud\n",
    "    list_of_words = slcv.vocabulary_.keys()\n",
    "    list_of_words = list(list_of_words)\n",
    "    list_of_words.sort()\n",
    "\n",
    "    wordcloud = WordCloud(background_color=\"white\").generate(\" \".join(list_of_words))\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
